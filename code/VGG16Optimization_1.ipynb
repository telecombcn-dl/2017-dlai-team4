{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv\n",
      "sample_submission.csv\n",
      "test\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "# dontFreezeLast = 0;\n",
    "\n",
    "# patience = 10;\n",
    "\n",
    "# gpuName = '/device:GPU:0'\n",
    "# workers = 2;\n",
    "# histogram_freq = 0;\n",
    "\n",
    "# epochs = 100;\n",
    "# validation_size=0.3;\n",
    "\n",
    "modelPath = '../models/VGG16_opt/run1.h5';\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the csv's so we can see some more information on the filenames and breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../input/labels.csv')\n",
    "# df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "# print('Training images: ',df_train.shape[0])\n",
    "# print('Test images: ',df_test.shape[0])\n",
    "\n",
    "# reduce dimensionality\n",
    "#df_train = df_train.head(100)\n",
    "#df_test = df_test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the breed needs to be one-hot encoded for the final submission, so we will now do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_series = pd.Series(df_train['breed'])\n",
    "# one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read in all of the images for test and train, using a for loop through the values of the csv files. I have also set an im_size variable which sets the size for the image to be re-sized to, 90x90 px, you should play with this number to see how it affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0 \n",
    "# for f, breed in tqdm(df_train.values[:10]):\n",
    "#     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "#     label = one_hot_labels[i]\n",
    "#     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "#     y_train.append(label)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in tqdm(df_test['id'].values):\n",
    "#     img = cv2.imread('../input/test/{}.jpg'.format(f))\n",
    "#     x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_raw = np.array(y_train, np.uint8)\n",
    "# x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "# x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the outputs to make sure everyting went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_raw.shape)\n",
    "# print(y_train_raw.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that there are 120 different breeds. We can put this in a num_class variable below that can then be used when creating the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = y_train_raw.shape[1]\n",
    "# print('Number of classes: ', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create a validation set so that you can gauge the performance of your model on independent data, unseen to the model in training. We do this by splitting the current training set (x_train_raw) and the corresponding labels (y_train_raw) so that we set aside 30 % of the data at random and put these in validation sets (X_valid and Y_valid).\n",
    "\n",
    "* This split needs to be improved so that it contains images from every class, with 120 separate classes some can not be represented and so the validation score is not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=validation_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the CNN architecture. Here we are using a pre-trained model VGG19 which has already been trained to identify many different dog breeds (as well as a lot of other objects from the imagenet dataset see here for more information: http://image-net.org/about-overview). Unfortunately it doesn't seem possible to downlod the weights from within this kernel so make sure you set the weights argument to 'imagenet' and not None, as it currently is below.\n",
    "\n",
    "We then remove the final layer and instead replace it with a single dense layer with the number of nodes corresponding to the number of breed classes we have (120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print('Getting data')\n",
    "    df_train = pd.read_csv('../input/labels.csv')\n",
    "    df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "    \n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "    \n",
    "    im_size = 90\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    \n",
    "    i = 0 \n",
    "    for f, breed in tqdm(df_train.values):\n",
    "        img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "    \n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    num_class = y_train_raw.shape[1]\n",
    "    \n",
    "    print('Splitting into training/validation')\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
    "    \n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and model for hyperas\n",
    "\n",
    "def model(X_train,Y_train,X_valid,Y_valid):\n",
    "    print('Creating model')\n",
    "    base_model = VGG16(weights = 'imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(im_size, im_size, 3))\n",
    "\n",
    "    dropout = {{uniform(0.5,1)}};\n",
    "    layers = {{choice([0,1,2])}};\n",
    "    dontFreeze = {{choice(list(range(5+1)))}};\n",
    "    batchSize = {{choice([16,64,256])}};\n",
    "    \n",
    "    print()\n",
    "    print('dropout=',dropout)\n",
    "    print('layers=',layers)\n",
    "    print('dontFreeze=',dontFreeze)\n",
    "    print('batchSize=',batchSize)\n",
    "    print()\n",
    "    \n",
    "    stepsPerEpoch = round( len(X_train)/batchSize );\n",
    "    \n",
    "    # Add a new top layer\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    if layers>=2:\n",
    "        x = Dense(1024,activation='relu')(x)\n",
    "    if layers>=1:\n",
    "        x = Dense(512,activation='relu')(x)\n",
    "    # in any case:\n",
    "    predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "    # This is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # First: train only the top layers (which were randomly initialized)\n",
    "    for i in range(len(base_model.layers)-dontFreeze):\n",
    "        base_model.layers[i].trainable = False\n",
    "\n",
    "    # predetermined optimizer\n",
    "    lr=0.00020389590556056983;\n",
    "    beta_1=0.9453158868247398;\n",
    "    beta_2=0.9925872692991417;\n",
    "    decay=0.000821336141287018;\n",
    "    adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=adam, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks_list = [];\n",
    "    callbacks_list.append(keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=1));\n",
    "\n",
    "\n",
    "    # data augmentation & fitting\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True);\n",
    "    \n",
    "    model.fit_generator(\n",
    "        datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
    "        steps_per_epoch=stepsPerEpoch,\n",
    "        epochs=150,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid,Y_valid),\n",
    "        workers=2,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks_list)\n",
    "#     model.fit(X_train, Y_train,\n",
    "#       epochs=100,\n",
    "#       batch_size = batchSize,\n",
    "#       validation_data=(X_valid, Y_valid),\n",
    "#       verbose=1,\n",
    "#       callbacks=callbacks_list)\n",
    "\n",
    "    score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.vgg16 import VGG16\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from subprocess import check_output\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout': hp.uniform('dropout', 0.5,1),\n",
      "        'layers': hp.choice('layers', [0,1,2]),\n",
      "        'dontFreeze': hp.choice('dontFreeze', list(range(5+1))),\n",
      "        'batchSize': hp.choice('batchSize', [16,64,256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: print('Getting data')\n",
      "  3: df_train = pd.read_csv('../input/labels.csv')\n",
      "  4: df_test = pd.read_csv('../input/sample_submission.csv')\n",
      "  5: \n",
      "  6: targets_series = pd.Series(df_train['breed'])\n",
      "  7: one_hot = pd.get_dummies(targets_series, sparse = True)\n",
      "  8: one_hot_labels = np.asarray(one_hot)\n",
      "  9: \n",
      " 10: im_size = 90\n",
      " 11: x_train = []\n",
      " 12: y_train = []\n",
      " 13: x_test = []\n",
      " 14: \n",
      " 15: i = 0 \n",
      " 16: for f, breed in tqdm(df_train.values):\n",
      " 17:     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
      " 18:     label = one_hot_labels[i]\n",
      " 19:     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
      " 20:     y_train.append(label)\n",
      " 21:     i += 1\n",
      " 22: \n",
      " 23: y_train_raw = np.array(y_train, np.uint8)\n",
      " 24: x_train_raw = np.array(x_train, np.float32) / 255.\n",
      " 25: num_class = y_train_raw.shape[1]\n",
      " 26: \n",
      " 27: print('Splitting into training/validation')\n",
      " 28: X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
      " 29: \n",
      " 30: \n",
      " 31: \n",
      " 32: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     print('Creating model')\n",
      "   4:     base_model = VGG16(weights = 'imagenet',\n",
      "   5:                        include_top=False,\n",
      "   6:                        input_shape=(im_size, im_size, 3))\n",
      "   7: \n",
      "   8:     dropout = space['dropout'];\n",
      "   9:     layers = space['layers'];\n",
      "  10:     dontFreeze = space['dontFreeze'];\n",
      "  11:     batchSize = space['batchSize'];\n",
      "  12:     \n",
      "  13:     print()\n",
      "  14:     print('dropout=',dropout)\n",
      "  15:     print('layers=',layers)\n",
      "  16:     print('dontFreeze=',dontFreeze)\n",
      "  17:     print('batchSize=',batchSize)\n",
      "  18:     print()\n",
      "  19:     \n",
      "  20:     stepsPerEpoch = round( len(X_train)/batchSize );\n",
      "  21:     \n",
      "  22:     # Add a new top layer\n",
      "  23:     x = base_model.output\n",
      "  24:     x = Flatten()(x)\n",
      "  25:     x = Dropout(dropout)(x)\n",
      "  26:     if layers>=2:\n",
      "  27:         x = Dense(1024,activation='relu')(x)\n",
      "  28:     if layers>=1:\n",
      "  29:         x = Dense(512,activation='relu')(x)\n",
      "  30:     # in any case:\n",
      "  31:     predictions = Dense(num_class, activation='softmax')(x)\n",
      "  32: \n",
      "  33:     # This is the model we will train\n",
      "  34:     model = Model(inputs=base_model.input, outputs=predictions)\n",
      "  35: \n",
      "  36:     # First: train only the top layers (which were randomly initialized)\n",
      "  37:     for i in range(len(base_model.layers)-dontFreeze):\n",
      "  38:         base_model.layers[i].trainable = False\n",
      "  39: \n",
      "  40:     # predetermined optimizer\n",
      "  41:     lr=0.00020389590556056983;\n",
      "  42:     beta_1=0.9453158868247398;\n",
      "  43:     beta_2=0.9925872692991417;\n",
      "  44:     decay=0.000821336141287018;\n",
      "  45:     adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
      "  46:     model.compile(loss='categorical_crossentropy', \n",
      "  47:                   optimizer=adam, \n",
      "  48:                   metrics=['accuracy'])\n",
      "  49: \n",
      "  50:     callbacks_list = [];\n",
      "  51:     callbacks_list.append(keras.callbacks.EarlyStopping(\n",
      "  52:         monitor='val_acc',\n",
      "  53:         patience=10,\n",
      "  54:         verbose=1));\n",
      "  55: \n",
      "  56: \n",
      "  57:     # data augmentation & fitting\n",
      "  58:     datagen = ImageDataGenerator(\n",
      "  59:         rotation_range=30,\n",
      "  60:         width_shift_range=0.1,\n",
      "  61:         height_shift_range=0.1,\n",
      "  62:         shear_range=0.5,\n",
      "  63:         zoom_range=0.5,\n",
      "  64:         horizontal_flip=True,\n",
      "  65:         vertical_flip=True);\n",
      "  66:     \n",
      "  67:     model.fit_generator(\n",
      "  68:         datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
      "  69:         steps_per_epoch=stepsPerEpoch,\n",
      "  70:         epochs=150,\n",
      "  71:         verbose=1,\n",
      "  72:         validation_data=(X_valid,Y_valid),\n",
      "  73:         workers=2,\n",
      "  74:         shuffle=True,\n",
      "  75:         callbacks=callbacks_list)\n",
      "  76: #     model.fit(X_train, Y_train,\n",
      "  77: #       epochs=100,\n",
      "  78: #       batch_size = batchSize,\n",
      "  79: #       validation_data=(X_valid, Y_valid),\n",
      "  80: #       verbose=1,\n",
      "  81: #       callbacks=callbacks_list)\n",
      "  82: \n",
      "  83:     score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
      "  84:     print('Test accuracy:', acc)\n",
      "  85:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  86: \n",
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:16<00:00, 133.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training/validation\n",
      "Creating model\n",
      "\n",
      "dropout= 0.6064002165637792\n",
      "layers= 2\n",
      "dontFreeze= 3\n",
      "batchSize= 64\n",
      "\n",
      "Epoch 1/150\n",
      "112/112 [==============================] - 36s 321ms/step - loss: 4.8042 - acc: 0.0077 - val_loss: 4.7782 - val_acc: 0.0173\n",
      "Epoch 2/150\n",
      "112/112 [==============================] - 23s 210ms/step - loss: 4.7648 - acc: 0.0125 - val_loss: 4.7111 - val_acc: 0.0241\n",
      "Epoch 3/150\n",
      "112/112 [==============================] - 23s 209ms/step - loss: 4.7141 - acc: 0.0166 - val_loss: 4.6349 - val_acc: 0.0241\n",
      "Epoch 4/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.6727 - acc: 0.0212 - val_loss: 4.5787 - val_acc: 0.0326\n",
      "Epoch 5/150\n",
      "112/112 [==============================] - 24s 212ms/step - loss: 4.6441 - acc: 0.0253 - val_loss: 4.5590 - val_acc: 0.0313\n",
      "Epoch 6/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 4.6001 - acc: 0.0267 - val_loss: 4.4830 - val_acc: 0.0300\n",
      "Epoch 7/150\n",
      "112/112 [==============================] - 24s 212ms/step - loss: 4.5717 - acc: 0.0342 - val_loss: 4.4438 - val_acc: 0.0391\n",
      "Epoch 8/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 4.5462 - acc: 0.0352 - val_loss: 4.3936 - val_acc: 0.0417\n",
      "Epoch 9/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 4.4871 - acc: 0.0428 - val_loss: 4.3272 - val_acc: 0.0427\n",
      "Epoch 10/150\n",
      "112/112 [==============================] - 28s 246ms/step - loss: 4.4423 - acc: 0.0430 - val_loss: 4.2833 - val_acc: 0.0518\n",
      "Epoch 11/150\n",
      "112/112 [==============================] - 28s 247ms/step - loss: 4.3980 - acc: 0.0473 - val_loss: 4.2060 - val_acc: 0.0642\n",
      "Epoch 12/150\n",
      "112/112 [==============================] - 26s 229ms/step - loss: 4.3437 - acc: 0.0524 - val_loss: 4.1417 - val_acc: 0.0646\n",
      "Epoch 13/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.2994 - acc: 0.0536 - val_loss: 4.0895 - val_acc: 0.0747\n",
      "Epoch 14/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 4.2615 - acc: 0.0591 - val_loss: 4.0678 - val_acc: 0.0809\n",
      "Epoch 15/150\n",
      "112/112 [==============================] - 26s 234ms/step - loss: 4.1932 - acc: 0.0616 - val_loss: 3.9812 - val_acc: 0.0913\n",
      "Epoch 16/150\n",
      "112/112 [==============================] - 27s 243ms/step - loss: 4.1561 - acc: 0.0689 - val_loss: 3.9337 - val_acc: 0.0933\n",
      "Epoch 17/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 4.1111 - acc: 0.0736 - val_loss: 3.9070 - val_acc: 0.0981\n",
      "Epoch 18/150\n",
      "112/112 [==============================] - 28s 251ms/step - loss: 4.0543 - acc: 0.0740 - val_loss: 3.8959 - val_acc: 0.0968\n",
      "Epoch 19/150\n",
      "112/112 [==============================] - 29s 258ms/step - loss: 4.0333 - acc: 0.0831 - val_loss: 3.8472 - val_acc: 0.1030\n",
      "Epoch 20/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 4.0173 - acc: 0.0826 - val_loss: 3.8381 - val_acc: 0.1118\n",
      "Epoch 21/150\n",
      "112/112 [==============================] - 28s 252ms/step - loss: 3.9687 - acc: 0.0905 - val_loss: 3.7672 - val_acc: 0.1184\n",
      "Epoch 22/150\n",
      "112/112 [==============================] - 28s 248ms/step - loss: 3.9277 - acc: 0.0921 - val_loss: 3.7311 - val_acc: 0.1246\n",
      "Epoch 23/150\n",
      "112/112 [==============================] - 27s 238ms/step - loss: 3.9110 - acc: 0.0960 - val_loss: 3.7085 - val_acc: 0.1330\n",
      "Epoch 24/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 3.8896 - acc: 0.0994 - val_loss: 3.6907 - val_acc: 0.1360\n",
      "Epoch 25/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 3.8551 - acc: 0.1086 - val_loss: 3.7099 - val_acc: 0.1265\n",
      "Epoch 26/150\n",
      "112/112 [==============================] - 26s 231ms/step - loss: 3.8518 - acc: 0.1039 - val_loss: 3.6386 - val_acc: 0.1454\n",
      "Epoch 27/150\n",
      "112/112 [==============================] - 26s 236ms/step - loss: 3.8004 - acc: 0.1150 - val_loss: 3.6341 - val_acc: 0.1435\n",
      "Epoch 28/150\n",
      "112/112 [==============================] - 28s 246ms/step - loss: 3.8010 - acc: 0.1186 - val_loss: 3.6400 - val_acc: 0.1500\n",
      "Epoch 29/150\n",
      "112/112 [==============================] - 28s 247ms/step - loss: 3.7764 - acc: 0.1204 - val_loss: 3.5982 - val_acc: 0.1493\n",
      "Epoch 30/150\n",
      "112/112 [==============================] - 29s 260ms/step - loss: 3.7562 - acc: 0.1166 - val_loss: 3.6017 - val_acc: 0.1545\n",
      "Epoch 31/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 3.7132 - acc: 0.1280 - val_loss: 3.5694 - val_acc: 0.1519\n",
      "Epoch 32/150\n",
      "112/112 [==============================] - 28s 254ms/step - loss: 3.7135 - acc: 0.1275 - val_loss: 3.5590 - val_acc: 0.1539\n",
      "Epoch 33/150\n",
      "112/112 [==============================] - 27s 245ms/step - loss: 3.7121 - acc: 0.1285 - val_loss: 3.5589 - val_acc: 0.1461\n",
      "Epoch 34/150\n",
      "112/112 [==============================] - 27s 240ms/step - loss: 3.6769 - acc: 0.1359 - val_loss: 3.4961 - val_acc: 0.1702\n",
      "Epoch 35/150\n",
      "112/112 [==============================] - 26s 235ms/step - loss: 3.6541 - acc: 0.1417 - val_loss: 3.4932 - val_acc: 0.1627\n",
      "Epoch 36/150\n",
      "112/112 [==============================] - 25s 224ms/step - loss: 3.6592 - acc: 0.1333 - val_loss: 3.5336 - val_acc: 0.1575\n",
      "Epoch 37/150\n",
      "112/112 [==============================] - 25s 226ms/step - loss: 3.6481 - acc: 0.1382 - val_loss: 3.5194 - val_acc: 0.1611\n",
      "Epoch 38/150\n",
      "112/112 [==============================] - 26s 230ms/step - loss: 3.6266 - acc: 0.1442 - val_loss: 3.5356 - val_acc: 0.1585\n",
      "Epoch 39/150\n",
      "112/112 [==============================] - 27s 243ms/step - loss: 3.6113 - acc: 0.1442 - val_loss: 3.4760 - val_acc: 0.1650\n",
      "Epoch 40/150\n",
      "112/112 [==============================] - 27s 242ms/step - loss: 3.6121 - acc: 0.1424 - val_loss: 3.4824 - val_acc: 0.1637\n",
      "Epoch 41/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.5649 - acc: 0.1509 - val_loss: 3.4702 - val_acc: 0.1689\n",
      "Epoch 42/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 3.5827 - acc: 0.1497 - val_loss: 3.4302 - val_acc: 0.1735\n",
      "Epoch 43/150\n",
      "112/112 [==============================] - 29s 262ms/step - loss: 3.5626 - acc: 0.1499 - val_loss: 3.4441 - val_acc: 0.1790\n",
      "Epoch 44/150\n",
      "112/112 [==============================] - 29s 261ms/step - loss: 3.5311 - acc: 0.1561 - val_loss: 3.4369 - val_acc: 0.1738\n",
      "Epoch 45/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.5176 - acc: 0.1631 - val_loss: 3.3832 - val_acc: 0.1826\n",
      "Epoch 46/150\n",
      "112/112 [==============================] - 27s 243ms/step - loss: 3.5093 - acc: 0.1591 - val_loss: 3.4091 - val_acc: 0.1777\n",
      "Epoch 47/150\n",
      "112/112 [==============================] - 27s 238ms/step - loss: 3.5018 - acc: 0.1692 - val_loss: 3.3938 - val_acc: 0.1806\n",
      "Epoch 48/150\n",
      "112/112 [==============================] - 26s 232ms/step - loss: 3.4835 - acc: 0.1675 - val_loss: 3.3856 - val_acc: 0.1911\n",
      "Epoch 49/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.4691 - acc: 0.1721 - val_loss: 3.3692 - val_acc: 0.1911\n",
      "Epoch 50/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 3.4510 - acc: 0.1715 - val_loss: 3.3405 - val_acc: 0.1927\n",
      "Epoch 51/150\n",
      "112/112 [==============================] - 26s 235ms/step - loss: 3.4436 - acc: 0.1761 - val_loss: 3.3853 - val_acc: 0.1878\n",
      "Epoch 52/150\n",
      "112/112 [==============================] - 27s 244ms/step - loss: 3.4491 - acc: 0.1704 - val_loss: 3.3521 - val_acc: 0.1943\n",
      "Epoch 53/150\n",
      "112/112 [==============================] - 28s 251ms/step - loss: 3.4327 - acc: 0.1780 - val_loss: 3.3496 - val_acc: 0.1940\n",
      "Epoch 54/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 3.4437 - acc: 0.1718 - val_loss: 3.3507 - val_acc: 0.1966\n",
      "Epoch 55/150\n",
      "112/112 [==============================] - 29s 255ms/step - loss: 3.4230 - acc: 0.1756 - val_loss: 3.3531 - val_acc: 0.1973\n",
      "Epoch 56/150\n",
      "112/112 [==============================] - 29s 262ms/step - loss: 3.4033 - acc: 0.1813 - val_loss: 3.3838 - val_acc: 0.1868\n",
      "Epoch 57/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.4128 - acc: 0.1826 - val_loss: 3.3181 - val_acc: 0.1969\n",
      "Epoch 58/150\n",
      "112/112 [==============================] - 28s 247ms/step - loss: 3.3864 - acc: 0.1857 - val_loss: 3.3283 - val_acc: 0.2025\n",
      "Epoch 59/150\n",
      "112/112 [==============================] - 26s 236ms/step - loss: 3.3982 - acc: 0.1844 - val_loss: 3.3383 - val_acc: 0.1950\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 25s 227ms/step - loss: 3.3556 - acc: 0.1880 - val_loss: 3.3290 - val_acc: 0.1982\n",
      "Epoch 61/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 3.3649 - acc: 0.1867 - val_loss: 3.3031 - val_acc: 0.2008\n",
      "Epoch 62/150\n",
      "112/112 [==============================] - 25s 227ms/step - loss: 3.3555 - acc: 0.1915 - val_loss: 3.3226 - val_acc: 0.2022\n",
      "Epoch 63/150\n",
      "112/112 [==============================] - 27s 238ms/step - loss: 3.3577 - acc: 0.1853 - val_loss: 3.3053 - val_acc: 0.2038\n",
      "Epoch 64/150\n",
      "112/112 [==============================] - 27s 240ms/step - loss: 3.3291 - acc: 0.1884 - val_loss: 3.3195 - val_acc: 0.2031\n",
      "Epoch 65/150\n",
      "112/112 [==============================] - 28s 254ms/step - loss: 3.3191 - acc: 0.1977 - val_loss: 3.2892 - val_acc: 0.2044\n",
      "Epoch 66/150\n",
      "112/112 [==============================] - 29s 259ms/step - loss: 3.3078 - acc: 0.1954 - val_loss: 3.3087 - val_acc: 0.2022\n",
      "Epoch 67/150\n",
      "112/112 [==============================] - 30s 270ms/step - loss: 3.3055 - acc: 0.1930 - val_loss: 3.2785 - val_acc: 0.2057\n",
      "Epoch 68/150\n",
      "112/112 [==============================] - 29s 260ms/step - loss: 3.2923 - acc: 0.2036 - val_loss: 3.2877 - val_acc: 0.2070\n",
      "Epoch 69/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 3.2919 - acc: 0.1997 - val_loss: 3.3055 - val_acc: 0.2090\n",
      "Epoch 70/150\n",
      "112/112 [==============================] - 28s 249ms/step - loss: 3.2753 - acc: 0.2041 - val_loss: 3.2636 - val_acc: 0.2097\n",
      "Epoch 71/150\n",
      "112/112 [==============================] - 28s 249ms/step - loss: 3.2775 - acc: 0.2038 - val_loss: 3.2825 - val_acc: 0.2087\n",
      "Epoch 72/150\n",
      "112/112 [==============================] - 27s 238ms/step - loss: 3.2733 - acc: 0.2021 - val_loss: 3.2434 - val_acc: 0.2087\n",
      "Epoch 73/150\n",
      "112/112 [==============================] - 25s 222ms/step - loss: 3.2531 - acc: 0.2029 - val_loss: 3.2680 - val_acc: 0.2077\n",
      "Epoch 74/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 3.2793 - acc: 0.2028 - val_loss: 3.2858 - val_acc: 0.2070\n",
      "Epoch 75/150\n",
      "112/112 [==============================] - 26s 231ms/step - loss: 3.2418 - acc: 0.2114 - val_loss: 3.2926 - val_acc: 0.2051\n",
      "Epoch 76/150\n",
      "112/112 [==============================] - 27s 242ms/step - loss: 3.2699 - acc: 0.2001 - val_loss: 3.2791 - val_acc: 0.2044\n",
      "Epoch 77/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.2320 - acc: 0.2117 - val_loss: 3.2627 - val_acc: 0.2093\n",
      "Epoch 78/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.2388 - acc: 0.2122 - val_loss: 3.2428 - val_acc: 0.2106\n",
      "Epoch 79/150\n",
      "112/112 [==============================] - 28s 251ms/step - loss: 3.2241 - acc: 0.2119 - val_loss: 3.2388 - val_acc: 0.2087\n",
      "Epoch 80/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 3.2037 - acc: 0.2201 - val_loss: 3.2599 - val_acc: 0.2129\n",
      "Epoch 81/150\n",
      "112/112 [==============================] - 29s 256ms/step - loss: 3.2135 - acc: 0.2152 - val_loss: 3.2201 - val_acc: 0.2132\n",
      "Epoch 82/150\n",
      "112/112 [==============================] - 28s 248ms/step - loss: 3.1800 - acc: 0.2237 - val_loss: 3.2690 - val_acc: 0.2103\n",
      "Epoch 83/150\n",
      "112/112 [==============================] - 28s 246ms/step - loss: 3.1882 - acc: 0.2194 - val_loss: 3.2188 - val_acc: 0.2175\n",
      "Epoch 84/150\n",
      "112/112 [==============================] - 26s 234ms/step - loss: 3.1957 - acc: 0.2116 - val_loss: 3.2688 - val_acc: 0.2106\n",
      "Epoch 85/150\n",
      "112/112 [==============================] - 25s 224ms/step - loss: 3.1774 - acc: 0.2188 - val_loss: 3.2538 - val_acc: 0.2080\n",
      "Epoch 86/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 3.1915 - acc: 0.2165 - val_loss: 3.2512 - val_acc: 0.2106\n",
      "Epoch 87/150\n",
      "112/112 [==============================] - 26s 232ms/step - loss: 3.1777 - acc: 0.2315 - val_loss: 3.2694 - val_acc: 0.2061\n",
      "Epoch 88/150\n",
      "112/112 [==============================] - 27s 242ms/step - loss: 3.1573 - acc: 0.2193 - val_loss: 3.2418 - val_acc: 0.2155\n",
      "Epoch 89/150\n",
      "112/112 [==============================] - 28s 251ms/step - loss: 3.1891 - acc: 0.2232 - val_loss: 3.2363 - val_acc: 0.2168\n",
      "Epoch 90/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 3.1572 - acc: 0.2272 - val_loss: 3.2060 - val_acc: 0.2185\n",
      "Epoch 91/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 3.1426 - acc: 0.2188 - val_loss: 3.2195 - val_acc: 0.2237\n",
      "Epoch 92/150\n",
      "112/112 [==============================] - 28s 252ms/step - loss: 3.1339 - acc: 0.2295 - val_loss: 3.1942 - val_acc: 0.2253\n",
      "Epoch 93/150\n",
      "112/112 [==============================] - 27s 244ms/step - loss: 3.1332 - acc: 0.2280 - val_loss: 3.2254 - val_acc: 0.2149\n",
      "Epoch 94/150\n",
      "112/112 [==============================] - 26s 229ms/step - loss: 3.1436 - acc: 0.2241 - val_loss: 3.1994 - val_acc: 0.2217\n",
      "Epoch 95/150\n",
      "112/112 [==============================] - 24s 215ms/step - loss: 3.0894 - acc: 0.2366 - val_loss: 3.2411 - val_acc: 0.2178\n",
      "Epoch 96/150\n",
      "112/112 [==============================] - 26s 230ms/step - loss: 3.1164 - acc: 0.2241 - val_loss: 3.2154 - val_acc: 0.2211\n",
      "Epoch 97/150\n",
      "112/112 [==============================] - 26s 237ms/step - loss: 3.1239 - acc: 0.2274 - val_loss: 3.2039 - val_acc: 0.2217\n",
      "Epoch 98/150\n",
      "112/112 [==============================] - 27s 245ms/step - loss: 3.1015 - acc: 0.2311 - val_loss: 3.1813 - val_acc: 0.2256\n",
      "Epoch 99/150\n",
      "112/112 [==============================] - 28s 254ms/step - loss: 3.0951 - acc: 0.2361 - val_loss: 3.1992 - val_acc: 0.2233\n",
      "Epoch 100/150\n",
      "112/112 [==============================] - 29s 258ms/step - loss: 3.0794 - acc: 0.2338 - val_loss: 3.1974 - val_acc: 0.2289\n",
      "Epoch 101/150\n",
      "112/112 [==============================] - 28s 253ms/step - loss: 3.1067 - acc: 0.2366 - val_loss: 3.1839 - val_acc: 0.2256\n",
      "Epoch 102/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 3.1000 - acc: 0.2339 - val_loss: 3.2395 - val_acc: 0.2204\n",
      "Epoch 103/150\n",
      "112/112 [==============================] - 27s 245ms/step - loss: 3.0655 - acc: 0.2390 - val_loss: 3.1930 - val_acc: 0.2263\n",
      "Epoch 104/150\n",
      "112/112 [==============================] - 26s 236ms/step - loss: 3.0621 - acc: 0.2362 - val_loss: 3.2096 - val_acc: 0.2266\n",
      "Epoch 105/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 3.0947 - acc: 0.2401 - val_loss: 3.1935 - val_acc: 0.2315\n",
      "Epoch 106/150\n",
      "112/112 [==============================] - 25s 224ms/step - loss: 3.0741 - acc: 0.2444 - val_loss: 3.2217 - val_acc: 0.2237\n",
      "Epoch 107/150\n",
      "112/112 [==============================] - 26s 232ms/step - loss: 3.0496 - acc: 0.2430 - val_loss: 3.2095 - val_acc: 0.2250\n",
      "Epoch 108/150\n",
      "112/112 [==============================] - 28s 252ms/step - loss: 3.0520 - acc: 0.2450 - val_loss: 3.2040 - val_acc: 0.2295\n",
      "Epoch 109/150\n",
      "112/112 [==============================] - 27s 240ms/step - loss: 3.0408 - acc: 0.2472 - val_loss: 3.1915 - val_acc: 0.2295\n",
      "Epoch 110/150\n",
      "112/112 [==============================] - 25s 226ms/step - loss: 3.0538 - acc: 0.2473 - val_loss: 3.1738 - val_acc: 0.2331\n",
      "Epoch 111/150\n",
      "112/112 [==============================] - 27s 237ms/step - loss: 3.0255 - acc: 0.2539 - val_loss: 3.2220 - val_acc: 0.2217\n",
      "Epoch 112/150\n",
      "112/112 [==============================] - 27s 241ms/step - loss: 3.0479 - acc: 0.2473 - val_loss: 3.2162 - val_acc: 0.2224\n",
      "Epoch 113/150\n",
      "112/112 [==============================] - 28s 249ms/step - loss: 3.0200 - acc: 0.2519 - val_loss: 3.2044 - val_acc: 0.2266\n",
      "Epoch 114/150\n",
      "112/112 [==============================] - 29s 256ms/step - loss: 3.0327 - acc: 0.2487 - val_loss: 3.1910 - val_acc: 0.2299\n",
      "Epoch 115/150\n",
      "112/112 [==============================] - 27s 244ms/step - loss: 3.0251 - acc: 0.2422 - val_loss: 3.1995 - val_acc: 0.2318\n",
      "Epoch 116/150\n",
      "112/112 [==============================] - 27s 239ms/step - loss: 3.0313 - acc: 0.2435 - val_loss: 3.1980 - val_acc: 0.2328\n",
      "Epoch 117/150\n",
      "112/112 [==============================] - 27s 239ms/step - loss: 3.0275 - acc: 0.2473 - val_loss: 3.2430 - val_acc: 0.2312\n",
      "Epoch 118/150\n",
      "112/112 [==============================] - 27s 239ms/step - loss: 2.9989 - acc: 0.2529 - val_loss: 3.1956 - val_acc: 0.2348\n",
      "Epoch 119/150\n",
      "112/112 [==============================] - 26s 232ms/step - loss: 2.9997 - acc: 0.2548 - val_loss: 3.1715 - val_acc: 0.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 2.9985 - acc: 0.2547 - val_loss: 3.2095 - val_acc: 0.2299\n",
      "Epoch 121/150\n",
      "112/112 [==============================] - 27s 237ms/step - loss: 2.9824 - acc: 0.2535 - val_loss: 3.1841 - val_acc: 0.2321\n",
      "Epoch 122/150\n",
      "112/112 [==============================] - 27s 243ms/step - loss: 3.0191 - acc: 0.2496 - val_loss: 3.2041 - val_acc: 0.2312\n",
      "Epoch 123/150\n",
      "112/112 [==============================] - 28s 250ms/step - loss: 3.0202 - acc: 0.2475 - val_loss: 3.2348 - val_acc: 0.2276\n",
      "Epoch 124/150\n",
      "112/112 [==============================] - 29s 257ms/step - loss: 2.9895 - acc: 0.2514 - val_loss: 3.1925 - val_acc: 0.2321\n",
      "Epoch 125/150\n",
      "112/112 [==============================] - 30s 263ms/step - loss: 2.9955 - acc: 0.2620 - val_loss: 3.2027 - val_acc: 0.2331\n",
      "Epoch 126/150\n",
      "112/112 [==============================] - 29s 263ms/step - loss: 2.9845 - acc: 0.2551 - val_loss: 3.2075 - val_acc: 0.2299\n",
      "Epoch 127/150\n",
      "112/112 [==============================] - 28s 254ms/step - loss: 2.9721 - acc: 0.2587 - val_loss: 3.1990 - val_acc: 0.2328\n",
      "Epoch 128/150\n",
      "112/112 [==============================] - 28s 251ms/step - loss: 2.9745 - acc: 0.2524 - val_loss: 3.1652 - val_acc: 0.2357\n",
      "Epoch 129/150\n",
      "112/112 [==============================] - 26s 234ms/step - loss: 2.9746 - acc: 0.2589 - val_loss: 3.1812 - val_acc: 0.2396\n",
      "Epoch 130/150\n",
      "112/112 [==============================] - 25s 222ms/step - loss: 2.9660 - acc: 0.2628 - val_loss: 3.2039 - val_acc: 0.2315\n",
      "Epoch 131/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 2.9543 - acc: 0.2666 - val_loss: 3.2197 - val_acc: 0.2328\n",
      "Epoch 132/150\n",
      "112/112 [==============================] - 27s 245ms/step - loss: 2.9481 - acc: 0.2577 - val_loss: 3.1937 - val_acc: 0.2348\n",
      "Epoch 133/150\n",
      "112/112 [==============================] - 26s 234ms/step - loss: 2.9744 - acc: 0.2561 - val_loss: 3.1779 - val_acc: 0.2357\n",
      "Epoch 134/150\n",
      "112/112 [==============================] - 27s 241ms/step - loss: 2.9511 - acc: 0.2596 - val_loss: 3.1868 - val_acc: 0.2390\n",
      "Epoch 135/150\n",
      "112/112 [==============================] - 26s 233ms/step - loss: 2.9383 - acc: 0.2678 - val_loss: 3.1912 - val_acc: 0.2383\n",
      "Epoch 136/150\n",
      "112/112 [==============================] - 26s 236ms/step - loss: 2.9242 - acc: 0.2631 - val_loss: 3.1796 - val_acc: 0.2364\n",
      "Epoch 137/150\n",
      "112/112 [==============================] - 26s 235ms/step - loss: 2.9468 - acc: 0.2666 - val_loss: 3.1998 - val_acc: 0.2374\n",
      "Epoch 138/150\n",
      "112/112 [==============================] - 28s 246ms/step - loss: 2.9497 - acc: 0.2623 - val_loss: 3.2242 - val_acc: 0.2361\n",
      "Epoch 139/150\n",
      "112/112 [==============================] - 29s 259ms/step - loss: 2.9249 - acc: 0.2613 - val_loss: 3.1983 - val_acc: 0.2357\n",
      "Epoch 00139: early stopping\n",
      "Test accuracy: 0.235735246115\n",
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:13<00:00, 139.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training/validation\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-380a415966d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           notebook_name='VGG16Optimization_1')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dddbeb5d4816>\u001b[0m in \u001b[0;36mdata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Splitting into training/validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=1,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='VGG16Optimization_1')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "val_loss, val_acc = best_model.evaluate(X_test, Y_test);\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(\"Validation loss: \", val_loss)\n",
    "print(\"Validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(modelPath);\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, accuracy is low here because we are not taking advantage of the pre-trained weights as they cannot be downloaded in the kernel. This means we are training the wights from scratch and I we have only run 1 epoch due to the hardware constraints in the kernel.\n",
    "\n",
    "Next we will make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.DataFrame(preds)\n",
    "# # Set column names to those generated by the one-hot encoding earlier\n",
    "# col_names = one_hot.columns.values\n",
    "# sub.columns = col_names\n",
    "# # Insert the column id from the sample_submission at the start of the data frame\n",
    "# sub.insert(0, 'id', df_test['id'])\n",
    "# sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
