{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv\n",
      "sample_submission.csv\n",
      "test\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "# dontFreezeLast = 0;\n",
    "\n",
    "# patience = 10;\n",
    "\n",
    "# gpuName = '/device:GPU:0'\n",
    "# workers = 2;\n",
    "# histogram_freq = 0;\n",
    "\n",
    "# epochs = 100;\n",
    "# validation_size=0.3;\n",
    "\n",
    "modelPath = '../models/VGG16_opt/'\n",
    "modelName = 'run1.h5';\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the csv's so we can see some more information on the filenames and breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../input/labels.csv')\n",
    "# df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "# print('Training images: ',df_train.shape[0])\n",
    "# print('Test images: ',df_test.shape[0])\n",
    "\n",
    "# reduce dimensionality\n",
    "#df_train = df_train.head(100)\n",
    "#df_test = df_test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the breed needs to be one-hot encoded for the final submission, so we will now do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_series = pd.Series(df_train['breed'])\n",
    "# one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read in all of the images for test and train, using a for loop through the values of the csv files. I have also set an im_size variable which sets the size for the image to be re-sized to, 90x90 px, you should play with this number to see how it affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0 \n",
    "# for f, breed in tqdm(df_train.values[:10]):\n",
    "#     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "#     label = one_hot_labels[i]\n",
    "#     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "#     y_train.append(label)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in tqdm(df_test['id'].values):\n",
    "#     img = cv2.imread('../input/test/{}.jpg'.format(f))\n",
    "#     x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_raw = np.array(y_train, np.uint8)\n",
    "# x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "# x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the outputs to make sure everyting went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_raw.shape)\n",
    "# print(y_train_raw.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that there are 120 different breeds. We can put this in a num_class variable below that can then be used when creating the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = y_train_raw.shape[1]\n",
    "# print('Number of classes: ', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create a validation set so that you can gauge the performance of your model on independent data, unseen to the model in training. We do this by splitting the current training set (x_train_raw) and the corresponding labels (y_train_raw) so that we set aside 30 % of the data at random and put these in validation sets (X_valid and Y_valid).\n",
    "\n",
    "* This split needs to be improved so that it contains images from every class, with 120 separate classes some can not be represented and so the validation score is not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=validation_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the CNN architecture. Here we are using a pre-trained model VGG19 which has already been trained to identify many different dog breeds (as well as a lot of other objects from the imagenet dataset see here for more information: http://image-net.org/about-overview). Unfortunately it doesn't seem possible to downlod the weights from within this kernel so make sure you set the weights argument to 'imagenet' and not None, as it currently is below.\n",
    "\n",
    "We then remove the final layer and instead replace it with a single dense layer with the number of nodes corresponding to the number of breed classes we have (120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print('Getting data')\n",
    "    df_train = pd.read_csv('../input/labels.csv')\n",
    "    df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "    \n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "    \n",
    "    im_size = 90\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    \n",
    "    i = 0 \n",
    "    for f, breed in tqdm(df_train.values):\n",
    "        img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "    \n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    num_class = y_train_raw.shape[1]\n",
    "    \n",
    "    print('Splitting into training/validation')\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
    "    \n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and model for hyperas\n",
    "\n",
    "def model(X_train,Y_train,X_valid,Y_valid):\n",
    "    print('Creating model')\n",
    "    base_model = VGG16(weights = 'imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(im_size, im_size, 3))\n",
    "\n",
    "    dropout = {{uniform(0.5,1)}};\n",
    "    layers = {{choice([0,1,2])}};\n",
    "    dontFreeze = {{choice(list(range(5+1)))}};\n",
    "    batchSize = {{choice([16,64,256])}};\n",
    "    \n",
    "    print()\n",
    "    print('dropout=',dropout)\n",
    "    print('layers=',layers)\n",
    "    print('dontFreeze=',dontFreeze)\n",
    "    print('batchSize=',batchSize)\n",
    "    print()\n",
    "    \n",
    "    stepsPerEpoch = round( len(X_train)/batchSize );\n",
    "    \n",
    "    # Add a new top layer\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    if layers>=2:\n",
    "        x = Dense(1024,activation='relu')(x)\n",
    "    if layers>=1:\n",
    "        x = Dense(512,activation='relu')(x)\n",
    "    # in any case:\n",
    "    predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "    # This is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # First: train only the top layers (which were randomly initialized)\n",
    "    for i in range(len(base_model.layers)-dontFreeze):\n",
    "        base_model.layers[i].trainable = False\n",
    "\n",
    "    # predetermined optimizer\n",
    "    lr=0.00020389590556056983;\n",
    "    beta_1=0.9453158868247398;\n",
    "    beta_2=0.9925872692991417;\n",
    "    decay=0.000821336141287018;\n",
    "    adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=adam, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks_list = [];\n",
    "    callbacks_list.append(keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=1));\n",
    "\n",
    "\n",
    "    # data augmentation & fitting\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True);\n",
    "    \n",
    "    model.fit_generator(\n",
    "        datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
    "        steps_per_epoch=stepsPerEpoch,\n",
    "        epochs=150,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid,Y_valid),\n",
    "        workers=2,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks_list)\n",
    "#     model.fit(X_train, Y_train,\n",
    "#       epochs=100,\n",
    "#       batch_size = batchSize,\n",
    "#       validation_data=(X_valid, Y_valid),\n",
    "#       verbose=1,\n",
    "#       callbacks=callbacks_list)\n",
    "\n",
    "    score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.vgg16 import VGG16\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from subprocess import check_output\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout': hp.uniform('dropout', 0.5,1),\n",
      "        'layers': hp.choice('layers', [0,1,2]),\n",
      "        'dontFreeze': hp.choice('dontFreeze', list(range(5+1))),\n",
      "        'batchSize': hp.choice('batchSize', [16,64,256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: print('Getting data')\n",
      "  3: df_train = pd.read_csv('../input/labels.csv')\n",
      "  4: df_test = pd.read_csv('../input/sample_submission.csv')\n",
      "  5: \n",
      "  6: targets_series = pd.Series(df_train['breed'])\n",
      "  7: one_hot = pd.get_dummies(targets_series, sparse = True)\n",
      "  8: one_hot_labels = np.asarray(one_hot)\n",
      "  9: \n",
      " 10: im_size = 90\n",
      " 11: x_train = []\n",
      " 12: y_train = []\n",
      " 13: x_test = []\n",
      " 14: \n",
      " 15: i = 0 \n",
      " 16: for f, breed in tqdm(df_train.values):\n",
      " 17:     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
      " 18:     label = one_hot_labels[i]\n",
      " 19:     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
      " 20:     y_train.append(label)\n",
      " 21:     i += 1\n",
      " 22: \n",
      " 23: y_train_raw = np.array(y_train, np.uint8)\n",
      " 24: x_train_raw = np.array(x_train, np.float32) / 255.\n",
      " 25: num_class = y_train_raw.shape[1]\n",
      " 26: \n",
      " 27: print('Splitting into training/validation')\n",
      " 28: X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
      " 29: \n",
      " 30: \n",
      " 31: \n",
      " 32: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     print('Creating model')\n",
      "   4:     base_model = VGG16(weights = 'imagenet',\n",
      "   5:                        include_top=False,\n",
      "   6:                        input_shape=(im_size, im_size, 3))\n",
      "   7: \n",
      "   8:     dropout = space['dropout'];\n",
      "   9:     layers = space['layers'];\n",
      "  10:     dontFreeze = space['dontFreeze'];\n",
      "  11:     batchSize = space['batchSize'];\n",
      "  12:     \n",
      "  13:     print()\n",
      "  14:     print('dropout=',dropout)\n",
      "  15:     print('layers=',layers)\n",
      "  16:     print('dontFreeze=',dontFreeze)\n",
      "  17:     print('batchSize=',batchSize)\n",
      "  18:     print()\n",
      "  19:     \n",
      "  20:     stepsPerEpoch = round( len(X_train)/batchSize );\n",
      "  21:     \n",
      "  22:     # Add a new top layer\n",
      "  23:     x = base_model.output\n",
      "  24:     x = Flatten()(x)\n",
      "  25:     x = Dropout(dropout)(x)\n",
      "  26:     if layers>=2:\n",
      "  27:         x = Dense(1024,activation='relu')(x)\n",
      "  28:     if layers>=1:\n",
      "  29:         x = Dense(512,activation='relu')(x)\n",
      "  30:     # in any case:\n",
      "  31:     predictions = Dense(num_class, activation='softmax')(x)\n",
      "  32: \n",
      "  33:     # This is the model we will train\n",
      "  34:     model = Model(inputs=base_model.input, outputs=predictions)\n",
      "  35: \n",
      "  36:     # First: train only the top layers (which were randomly initialized)\n",
      "  37:     for i in range(len(base_model.layers)-dontFreeze):\n",
      "  38:         base_model.layers[i].trainable = False\n",
      "  39: \n",
      "  40:     # predetermined optimizer\n",
      "  41:     lr=0.00020389590556056983;\n",
      "  42:     beta_1=0.9453158868247398;\n",
      "  43:     beta_2=0.9925872692991417;\n",
      "  44:     decay=0.000821336141287018;\n",
      "  45:     adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
      "  46:     model.compile(loss='categorical_crossentropy', \n",
      "  47:                   optimizer=adam, \n",
      "  48:                   metrics=['accuracy'])\n",
      "  49: \n",
      "  50:     callbacks_list = [];\n",
      "  51:     callbacks_list.append(keras.callbacks.EarlyStopping(\n",
      "  52:         monitor='val_acc',\n",
      "  53:         patience=10,\n",
      "  54:         verbose=1));\n",
      "  55: \n",
      "  56: \n",
      "  57:     # data augmentation & fitting\n",
      "  58:     datagen = ImageDataGenerator(\n",
      "  59:         rotation_range=30,\n",
      "  60:         width_shift_range=0.1,\n",
      "  61:         height_shift_range=0.1,\n",
      "  62:         shear_range=0.5,\n",
      "  63:         zoom_range=0.5,\n",
      "  64:         horizontal_flip=True,\n",
      "  65:         vertical_flip=True);\n",
      "  66:     \n",
      "  67:     model.fit_generator(\n",
      "  68:         datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
      "  69:         steps_per_epoch=stepsPerEpoch,\n",
      "  70:         epochs=150,\n",
      "  71:         verbose=1,\n",
      "  72:         validation_data=(X_valid,Y_valid),\n",
      "  73:         workers=2,\n",
      "  74:         shuffle=True,\n",
      "  75:         callbacks=callbacks_list)\n",
      "  76: #     model.fit(X_train, Y_train,\n",
      "  77: #       epochs=100,\n",
      "  78: #       batch_size = batchSize,\n",
      "  79: #       validation_data=(X_valid, Y_valid),\n",
      "  80: #       verbose=1,\n",
      "  81: #       callbacks=callbacks_list)\n",
      "  82: \n",
      "  83:     score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
      "  84:     print('Test accuracy:', acc)\n",
      "  85:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  86: \n",
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:30<00:00, 330.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training/validation\n",
      "Creating model\n",
      "\n",
      "dropout= 0.6064002165637792\n",
      "layers= 2\n",
      "dontFreeze= 3\n",
      "batchSize= 64\n",
      "\n",
      "Epoch 1/150\n",
      "112/112 [==============================] - 30s 264ms/step - loss: 4.8055 - acc: 0.0073 - val_loss: 4.7833 - val_acc: 0.0108\n",
      "Epoch 2/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.7843 - acc: 0.0101 - val_loss: 4.7840 - val_acc: 0.0153\n",
      "Epoch 3/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.7758 - acc: 0.0144 - val_loss: 4.7663 - val_acc: 0.0137\n",
      "Epoch 4/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.7706 - acc: 0.0158 - val_loss: 4.7538 - val_acc: 0.0147\n",
      "Epoch 5/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 4.7507 - acc: 0.0168 - val_loss: 4.7477 - val_acc: 0.0140\n",
      "Epoch 6/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.7437 - acc: 0.0155 - val_loss: 4.7288 - val_acc: 0.0170\n",
      "Epoch 7/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.7295 - acc: 0.0195 - val_loss: 4.7025 - val_acc: 0.0176\n",
      "Epoch 8/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.7069 - acc: 0.0166 - val_loss: 4.6899 - val_acc: 0.0179\n",
      "Epoch 9/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.6816 - acc: 0.0202 - val_loss: 4.6441 - val_acc: 0.0218\n",
      "Epoch 10/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 4.6571 - acc: 0.0211 - val_loss: 4.6108 - val_acc: 0.0231\n",
      "Epoch 11/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.6299 - acc: 0.0205 - val_loss: 4.5696 - val_acc: 0.0251\n",
      "Epoch 12/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.5985 - acc: 0.0247 - val_loss: 4.5271 - val_acc: 0.0274\n",
      "Epoch 13/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.5683 - acc: 0.0277 - val_loss: 4.4822 - val_acc: 0.0372\n",
      "Epoch 14/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.5358 - acc: 0.0287 - val_loss: 4.4381 - val_acc: 0.0349\n",
      "Epoch 15/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.5098 - acc: 0.0324 - val_loss: 4.3877 - val_acc: 0.0437\n",
      "Epoch 16/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.4650 - acc: 0.0339 - val_loss: 4.3505 - val_acc: 0.0391\n",
      "Epoch 17/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.4328 - acc: 0.0382 - val_loss: 4.3108 - val_acc: 0.0489\n",
      "Epoch 18/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.4078 - acc: 0.0385 - val_loss: 4.2701 - val_acc: 0.0466\n",
      "Epoch 19/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.3530 - acc: 0.0404 - val_loss: 4.2276 - val_acc: 0.0541\n",
      "Epoch 20/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.3360 - acc: 0.0456 - val_loss: 4.1937 - val_acc: 0.0584\n",
      "Epoch 21/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.2920 - acc: 0.0522 - val_loss: 4.1529 - val_acc: 0.0753\n",
      "Epoch 22/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.2652 - acc: 0.0526 - val_loss: 4.1114 - val_acc: 0.0867\n",
      "Epoch 23/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.2423 - acc: 0.0596 - val_loss: 4.0648 - val_acc: 0.0877\n",
      "Epoch 24/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.2064 - acc: 0.0612 - val_loss: 4.0327 - val_acc: 0.0897\n",
      "Epoch 25/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.1804 - acc: 0.0633 - val_loss: 3.9845 - val_acc: 0.0991\n",
      "Epoch 26/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.1390 - acc: 0.0727 - val_loss: 3.9521 - val_acc: 0.0975\n",
      "Epoch 27/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.1030 - acc: 0.0713 - val_loss: 3.9349 - val_acc: 0.0949\n",
      "Epoch 28/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.0824 - acc: 0.0748 - val_loss: 3.9036 - val_acc: 0.1034\n",
      "Epoch 29/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.0536 - acc: 0.0766 - val_loss: 3.8693 - val_acc: 0.1066\n",
      "Epoch 30/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.0545 - acc: 0.0782 - val_loss: 3.8275 - val_acc: 0.1141\n",
      "Epoch 31/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 4.0101 - acc: 0.0885 - val_loss: 3.8301 - val_acc: 0.1096\n",
      "Epoch 32/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.9753 - acc: 0.0915 - val_loss: 3.7934 - val_acc: 0.1148\n",
      "Epoch 33/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.9734 - acc: 0.0884 - val_loss: 3.7690 - val_acc: 0.1161\n",
      "Epoch 34/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.9391 - acc: 0.0943 - val_loss: 3.7624 - val_acc: 0.1193\n",
      "Epoch 35/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.9365 - acc: 0.0909 - val_loss: 3.7364 - val_acc: 0.1252\n",
      "Epoch 36/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.9054 - acc: 0.0948 - val_loss: 3.7571 - val_acc: 0.1180\n",
      "Epoch 37/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8939 - acc: 0.1009 - val_loss: 3.6892 - val_acc: 0.1272\n",
      "Epoch 38/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8934 - acc: 0.0976 - val_loss: 3.6957 - val_acc: 0.1278\n",
      "Epoch 39/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8574 - acc: 0.1042 - val_loss: 3.7030 - val_acc: 0.1317\n",
      "Epoch 40/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8518 - acc: 0.1052 - val_loss: 3.6593 - val_acc: 0.1373\n",
      "Epoch 41/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8226 - acc: 0.1079 - val_loss: 3.6248 - val_acc: 0.1418\n",
      "Epoch 42/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.8025 - acc: 0.1119 - val_loss: 3.6267 - val_acc: 0.1389\n",
      "Epoch 43/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7948 - acc: 0.1201 - val_loss: 3.6363 - val_acc: 0.1317\n",
      "Epoch 44/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7640 - acc: 0.1166 - val_loss: 3.6374 - val_acc: 0.1422\n",
      "Epoch 45/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7747 - acc: 0.1182 - val_loss: 3.5915 - val_acc: 0.1428\n",
      "Epoch 46/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.7526 - acc: 0.1168 - val_loss: 3.6152 - val_acc: 0.1474\n",
      "Epoch 47/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7570 - acc: 0.1199 - val_loss: 3.5878 - val_acc: 0.1441\n",
      "Epoch 48/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7366 - acc: 0.1186 - val_loss: 3.6219 - val_acc: 0.1454\n",
      "Epoch 49/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7295 - acc: 0.1282 - val_loss: 3.5571 - val_acc: 0.1519\n",
      "Epoch 50/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7300 - acc: 0.1222 - val_loss: 3.5671 - val_acc: 0.1435\n",
      "Epoch 51/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.7017 - acc: 0.1298 - val_loss: 3.5485 - val_acc: 0.1539\n",
      "Epoch 52/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6911 - acc: 0.1276 - val_loss: 3.5312 - val_acc: 0.1529\n",
      "Epoch 53/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6836 - acc: 0.1321 - val_loss: 3.5200 - val_acc: 0.1562\n",
      "Epoch 54/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6662 - acc: 0.1322 - val_loss: 3.5025 - val_acc: 0.1611\n",
      "Epoch 55/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6581 - acc: 0.1346 - val_loss: 3.5324 - val_acc: 0.1542\n",
      "Epoch 56/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6358 - acc: 0.1435 - val_loss: 3.4907 - val_acc: 0.1620\n",
      "Epoch 57/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6301 - acc: 0.1401 - val_loss: 3.5003 - val_acc: 0.1663\n",
      "Epoch 58/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6312 - acc: 0.1350 - val_loss: 3.4914 - val_acc: 0.1647\n",
      "Epoch 59/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5956 - acc: 0.1434 - val_loss: 3.4922 - val_acc: 0.1669\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6109 - acc: 0.1424 - val_loss: 3.4664 - val_acc: 0.1702\n",
      "Epoch 61/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.6021 - acc: 0.1473 - val_loss: 3.4701 - val_acc: 0.1725\n",
      "Epoch 62/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.5819 - acc: 0.1490 - val_loss: 3.4740 - val_acc: 0.1705\n",
      "Epoch 63/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.5731 - acc: 0.1515 - val_loss: 3.4506 - val_acc: 0.1738\n",
      "Epoch 64/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5669 - acc: 0.1540 - val_loss: 3.4437 - val_acc: 0.1767\n",
      "Epoch 65/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5570 - acc: 0.1520 - val_loss: 3.4397 - val_acc: 0.1813\n",
      "Epoch 66/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5572 - acc: 0.1561 - val_loss: 3.4509 - val_acc: 0.1728\n",
      "Epoch 67/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5382 - acc: 0.1630 - val_loss: 3.4287 - val_acc: 0.1865\n",
      "Epoch 68/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5454 - acc: 0.1580 - val_loss: 3.4688 - val_acc: 0.1748\n",
      "Epoch 69/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4984 - acc: 0.1601 - val_loss: 3.4074 - val_acc: 0.1858\n",
      "Epoch 70/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5185 - acc: 0.1535 - val_loss: 3.3972 - val_acc: 0.1826\n",
      "Epoch 71/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.5194 - acc: 0.1600 - val_loss: 3.4207 - val_acc: 0.1832\n",
      "Epoch 72/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4786 - acc: 0.1636 - val_loss: 3.3988 - val_acc: 0.1875\n",
      "Epoch 73/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.4865 - acc: 0.1674 - val_loss: 3.3822 - val_acc: 0.1904\n",
      "Epoch 74/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4847 - acc: 0.1657 - val_loss: 3.3672 - val_acc: 0.1930\n",
      "Epoch 75/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.4666 - acc: 0.1625 - val_loss: 3.3775 - val_acc: 0.1911\n",
      "Epoch 76/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4677 - acc: 0.1623 - val_loss: 3.3915 - val_acc: 0.1907\n",
      "Epoch 77/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4735 - acc: 0.1675 - val_loss: 3.3756 - val_acc: 0.1875\n",
      "Epoch 78/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.4643 - acc: 0.1710 - val_loss: 3.3531 - val_acc: 0.1933\n",
      "Epoch 79/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4378 - acc: 0.1790 - val_loss: 3.3909 - val_acc: 0.1907\n",
      "Epoch 80/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4258 - acc: 0.1756 - val_loss: 3.3429 - val_acc: 0.2022\n",
      "Epoch 81/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4234 - acc: 0.1745 - val_loss: 3.3591 - val_acc: 0.1986\n",
      "Epoch 82/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4153 - acc: 0.1731 - val_loss: 3.3706 - val_acc: 0.1960\n",
      "Epoch 83/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.4236 - acc: 0.1736 - val_loss: 3.3380 - val_acc: 0.1989\n",
      "Epoch 84/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3891 - acc: 0.1778 - val_loss: 3.3572 - val_acc: 0.1992\n",
      "Epoch 85/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.4106 - acc: 0.1782 - val_loss: 3.3426 - val_acc: 0.1986\n",
      "Epoch 86/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 3.4058 - acc: 0.1769 - val_loss: 3.3379 - val_acc: 0.2038\n",
      "Epoch 87/150\n",
      "112/112 [==============================] - 25s 223ms/step - loss: 3.3815 - acc: 0.1799 - val_loss: 3.3494 - val_acc: 0.1973\n",
      "Epoch 88/150\n",
      "112/112 [==============================] - 25s 222ms/step - loss: 3.4024 - acc: 0.1750 - val_loss: 3.3506 - val_acc: 0.1960\n",
      "Epoch 89/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.3870 - acc: 0.1827 - val_loss: 3.3034 - val_acc: 0.2087\n",
      "Epoch 90/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3534 - acc: 0.1920 - val_loss: 3.3425 - val_acc: 0.1986\n",
      "Epoch 91/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3510 - acc: 0.1836 - val_loss: 3.3189 - val_acc: 0.2048\n",
      "Epoch 92/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 3.3594 - acc: 0.1904 - val_loss: 3.3254 - val_acc: 0.2070\n",
      "Epoch 93/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 3.3530 - acc: 0.1828 - val_loss: 3.3612 - val_acc: 0.2012\n",
      "Epoch 94/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3438 - acc: 0.1884 - val_loss: 3.2894 - val_acc: 0.2074\n",
      "Epoch 95/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 3.3397 - acc: 0.1902 - val_loss: 3.3073 - val_acc: 0.2064\n",
      "Epoch 96/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3413 - acc: 0.1854 - val_loss: 3.2917 - val_acc: 0.2083\n",
      "Epoch 97/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3422 - acc: 0.1883 - val_loss: 3.2999 - val_acc: 0.2064\n",
      "Epoch 98/150\n",
      "112/112 [==============================] - 25s 219ms/step - loss: 3.3300 - acc: 0.1867 - val_loss: 3.2841 - val_acc: 0.2129\n",
      "Epoch 99/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3172 - acc: 0.1992 - val_loss: 3.3082 - val_acc: 0.2061\n",
      "Epoch 100/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3240 - acc: 0.1946 - val_loss: 3.3086 - val_acc: 0.2100\n",
      "Epoch 101/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.3046 - acc: 0.1940 - val_loss: 3.3129 - val_acc: 0.2061\n",
      "Epoch 102/150\n",
      "112/112 [==============================] - 26s 230ms/step - loss: 3.3070 - acc: 0.2009 - val_loss: 3.3095 - val_acc: 0.2080\n",
      "Epoch 103/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.3150 - acc: 0.1966 - val_loss: 3.3107 - val_acc: 0.2083\n",
      "Epoch 104/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2852 - acc: 0.1984 - val_loss: 3.3177 - val_acc: 0.2070\n",
      "Epoch 105/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.2783 - acc: 0.2007 - val_loss: 3.2783 - val_acc: 0.2129\n",
      "Epoch 106/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2599 - acc: 0.2092 - val_loss: 3.2838 - val_acc: 0.2139\n",
      "Epoch 107/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2622 - acc: 0.2010 - val_loss: 3.2919 - val_acc: 0.2132\n",
      "Epoch 108/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.2771 - acc: 0.1995 - val_loss: 3.2643 - val_acc: 0.2145\n",
      "Epoch 109/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2588 - acc: 0.2100 - val_loss: 3.2916 - val_acc: 0.2136\n",
      "Epoch 110/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2554 - acc: 0.2076 - val_loss: 3.2614 - val_acc: 0.2158\n",
      "Epoch 111/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2807 - acc: 0.2066 - val_loss: 3.2834 - val_acc: 0.2110\n",
      "Epoch 112/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2441 - acc: 0.2099 - val_loss: 3.3252 - val_acc: 0.2097\n",
      "Epoch 113/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.2524 - acc: 0.2045 - val_loss: 3.2951 - val_acc: 0.2106\n",
      "Epoch 114/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2404 - acc: 0.2079 - val_loss: 3.2838 - val_acc: 0.2123\n",
      "Epoch 115/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2345 - acc: 0.2130 - val_loss: 3.2812 - val_acc: 0.2106\n",
      "Epoch 116/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2240 - acc: 0.2102 - val_loss: 3.2710 - val_acc: 0.2139\n",
      "Epoch 117/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2066 - acc: 0.2104 - val_loss: 3.2656 - val_acc: 0.2158\n",
      "Epoch 118/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.2156 - acc: 0.2093 - val_loss: 3.2839 - val_acc: 0.2168\n",
      "Epoch 119/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1998 - acc: 0.2167 - val_loss: 3.2515 - val_acc: 0.2132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1997 - acc: 0.2123 - val_loss: 3.2598 - val_acc: 0.2136\n",
      "Epoch 121/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1868 - acc: 0.2267 - val_loss: 3.2967 - val_acc: 0.2119\n",
      "Epoch 122/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.2114 - acc: 0.2133 - val_loss: 3.3038 - val_acc: 0.2132\n",
      "Epoch 123/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.2013 - acc: 0.2158 - val_loss: 3.2250 - val_acc: 0.2224\n",
      "Epoch 124/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1652 - acc: 0.2206 - val_loss: 3.2658 - val_acc: 0.2172\n",
      "Epoch 125/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1805 - acc: 0.2153 - val_loss: 3.2428 - val_acc: 0.2172\n",
      "Epoch 126/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1609 - acc: 0.2233 - val_loss: 3.2863 - val_acc: 0.2139\n",
      "Epoch 127/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.1806 - acc: 0.2222 - val_loss: 3.2694 - val_acc: 0.2185\n",
      "Epoch 128/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1695 - acc: 0.2232 - val_loss: 3.2316 - val_acc: 0.2214\n",
      "Epoch 129/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1602 - acc: 0.2143 - val_loss: 3.2782 - val_acc: 0.2175\n",
      "Epoch 130/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1673 - acc: 0.2281 - val_loss: 3.2341 - val_acc: 0.2214\n",
      "Epoch 131/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1680 - acc: 0.2235 - val_loss: 3.2358 - val_acc: 0.2269\n",
      "Epoch 132/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.1571 - acc: 0.2293 - val_loss: 3.2687 - val_acc: 0.2185\n",
      "Epoch 133/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1499 - acc: 0.2176 - val_loss: 3.2420 - val_acc: 0.2188\n",
      "Epoch 134/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1315 - acc: 0.2283 - val_loss: 3.2148 - val_acc: 0.2263\n",
      "Epoch 135/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1339 - acc: 0.2269 - val_loss: 3.2492 - val_acc: 0.2207\n",
      "Epoch 136/150\n",
      "112/112 [==============================] - 25s 221ms/step - loss: 3.1413 - acc: 0.2282 - val_loss: 3.2615 - val_acc: 0.2214\n",
      "Epoch 137/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1420 - acc: 0.2223 - val_loss: 3.2479 - val_acc: 0.2201\n",
      "Epoch 138/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1364 - acc: 0.2264 - val_loss: 3.2568 - val_acc: 0.2136\n",
      "Epoch 139/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1239 - acc: 0.2296 - val_loss: 3.2292 - val_acc: 0.2233\n",
      "Epoch 140/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1386 - acc: 0.2257 - val_loss: 3.2595 - val_acc: 0.2185\n",
      "Epoch 141/150\n",
      "112/112 [==============================] - 25s 220ms/step - loss: 3.1194 - acc: 0.2329 - val_loss: 3.2693 - val_acc: 0.2172\n",
      "Epoch 00141: early stopping\n",
      "Test accuracy: 0.217150309695\n",
      "Creating model\n",
      "\n",
      "dropout= 0.6165806722373868\n",
      "layers= 0\n",
      "dontFreeze= 4\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 35s 1s/step - loss: 4.9043 - acc: 0.0081 - val_loss: 4.7874 - val_acc: 0.0108\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 22s 771ms/step - loss: 4.7873 - acc: 0.0116 - val_loss: 4.7873 - val_acc: 0.0104\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 22s 772ms/step - loss: 4.7871 - acc: 0.0116 - val_loss: 4.7872 - val_acc: 0.0104\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 22s 774ms/step - loss: 4.7869 - acc: 0.0116 - val_loss: 4.7871 - val_acc: 0.0104\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 22s 772ms/step - loss: 4.7868 - acc: 0.0116 - val_loss: 4.7870 - val_acc: 0.0104\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 22s 773ms/step - loss: 4.7866 - acc: 0.0116 - val_loss: 4.7869 - val_acc: 0.0104\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 22s 774ms/step - loss: 4.7865 - acc: 0.0116 - val_loss: 4.7868 - val_acc: 0.0104\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 22s 771ms/step - loss: 4.7863 - acc: 0.0116 - val_loss: 4.7867 - val_acc: 0.0104\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 22s 773ms/step - loss: 4.7862 - acc: 0.0116 - val_loss: 4.7866 - val_acc: 0.0104\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 22s 771ms/step - loss: 4.7860 - acc: 0.0116 - val_loss: 4.7866 - val_acc: 0.0104\n",
      "Epoch 11/150\n",
      "28/28 [==============================] - 22s 772ms/step - loss: 4.7859 - acc: 0.0116 - val_loss: 4.7865 - val_acc: 0.0104\n",
      "Epoch 00011: early stopping\n",
      "Test accuracy: 0.0104336485189\n",
      "Creating model\n",
      "\n",
      "dropout= 0.8814062151635532\n",
      "layers= 1\n",
      "dontFreeze= 4\n",
      "batchSize= 64\n",
      "\n",
      "Epoch 1/150\n",
      "112/112 [==============================] - 26s 235ms/step - loss: 4.8528 - acc: 0.0095 - val_loss: 4.7871 - val_acc: 0.0098\n",
      "Epoch 2/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7868 - acc: 0.0104 - val_loss: 4.7869 - val_acc: 0.0101\n",
      "Epoch 3/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7863 - acc: 0.0115 - val_loss: 4.7866 - val_acc: 0.0098\n",
      "Epoch 4/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7859 - acc: 0.0092 - val_loss: 4.7863 - val_acc: 0.0137\n",
      "Epoch 5/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7855 - acc: 0.0108 - val_loss: 4.7860 - val_acc: 0.0137\n",
      "Epoch 6/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7850 - acc: 0.0116 - val_loss: 4.7858 - val_acc: 0.0098\n",
      "Epoch 7/150\n",
      "112/112 [==============================] - 25s 227ms/step - loss: 4.7845 - acc: 0.0100 - val_loss: 4.7855 - val_acc: 0.0137\n",
      "Epoch 8/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7840 - acc: 0.0107 - val_loss: 4.7852 - val_acc: 0.0137\n",
      "Epoch 9/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7835 - acc: 0.0118 - val_loss: 4.7849 - val_acc: 0.0137\n",
      "Epoch 10/150\n",
      "112/112 [==============================] - 25s 228ms/step - loss: 4.7830 - acc: 0.0117 - val_loss: 4.7846 - val_acc: 0.0137\n",
      "Epoch 11/150\n",
      "112/112 [==============================] - 25s 228ms/step - loss: 4.7825 - acc: 0.0117 - val_loss: 4.7844 - val_acc: 0.0137\n",
      "Epoch 12/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7819 - acc: 0.0117 - val_loss: 4.7842 - val_acc: 0.0137\n",
      "Epoch 13/150\n",
      "112/112 [==============================] - 25s 227ms/step - loss: 4.7815 - acc: 0.0118 - val_loss: 4.7840 - val_acc: 0.0137\n",
      "Epoch 14/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.7810 - acc: 0.0117 - val_loss: 4.7838 - val_acc: 0.0137\n",
      "Epoch 00014: early stopping\n",
      "Test accuracy: 0.0136941636779\n",
      "Creating model\n",
      "\n",
      "dropout= 0.742222761866006\n",
      "layers= 1\n",
      "dontFreeze= 1\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 21s 751ms/step - loss: 5.2458 - acc: 0.0095 - val_loss: 4.8110 - val_acc: 0.0104\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 5.0228 - acc: 0.0095 - val_loss: 4.7775 - val_acc: 0.0160\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.9415 - acc: 0.0120 - val_loss: 4.7572 - val_acc: 0.0179\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.9095 - acc: 0.0112 - val_loss: 4.7472 - val_acc: 0.0196\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.8809 - acc: 0.0131 - val_loss: 4.7407 - val_acc: 0.0215\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.8512 - acc: 0.0152 - val_loss: 4.7350 - val_acc: 0.0248\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.8200 - acc: 0.0144 - val_loss: 4.7288 - val_acc: 0.0258\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.8171 - acc: 0.0174 - val_loss: 4.7244 - val_acc: 0.0280\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.7892 - acc: 0.0179 - val_loss: 4.7188 - val_acc: 0.0293\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.7815 - acc: 0.0173 - val_loss: 4.7114 - val_acc: 0.0329\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 20s 709ms/step - loss: 4.7753 - acc: 0.0178 - val_loss: 4.7028 - val_acc: 0.0323\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.7582 - acc: 0.0219 - val_loss: 4.6935 - val_acc: 0.0355\n",
      "Epoch 13/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.7549 - acc: 0.0196 - val_loss: 4.6850 - val_acc: 0.0355\n",
      "Epoch 14/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.7389 - acc: 0.0208 - val_loss: 4.6760 - val_acc: 0.0368\n",
      "Epoch 15/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.7387 - acc: 0.0243 - val_loss: 4.6668 - val_acc: 0.0375\n",
      "Epoch 16/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.7272 - acc: 0.0208 - val_loss: 4.6573 - val_acc: 0.0404\n",
      "Epoch 17/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.7064 - acc: 0.0274 - val_loss: 4.6437 - val_acc: 0.0437\n",
      "Epoch 18/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.7131 - acc: 0.0253 - val_loss: 4.6329 - val_acc: 0.0430\n",
      "Epoch 19/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.6957 - acc: 0.0289 - val_loss: 4.6192 - val_acc: 0.0466\n",
      "Epoch 20/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.6916 - acc: 0.0233 - val_loss: 4.6043 - val_acc: 0.0496\n",
      "Epoch 21/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.6776 - acc: 0.0271 - val_loss: 4.5916 - val_acc: 0.0499\n",
      "Epoch 22/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.6744 - acc: 0.0271 - val_loss: 4.5800 - val_acc: 0.0545\n",
      "Epoch 23/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.6633 - acc: 0.0275 - val_loss: 4.5679 - val_acc: 0.0564\n",
      "Epoch 24/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.6542 - acc: 0.0340 - val_loss: 4.5538 - val_acc: 0.0567\n",
      "Epoch 25/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.6466 - acc: 0.0331 - val_loss: 4.5399 - val_acc: 0.0580\n",
      "Epoch 26/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.6379 - acc: 0.0320 - val_loss: 4.5226 - val_acc: 0.0616\n",
      "Epoch 27/150\n",
      "28/28 [==============================] - 22s 803ms/step - loss: 4.6236 - acc: 0.0320 - val_loss: 4.5093 - val_acc: 0.0626\n",
      "Epoch 28/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.6266 - acc: 0.0338 - val_loss: 4.4985 - val_acc: 0.0619\n",
      "Epoch 29/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.6127 - acc: 0.0373 - val_loss: 4.4870 - val_acc: 0.0636\n",
      "Epoch 30/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5945 - acc: 0.0380 - val_loss: 4.4729 - val_acc: 0.0646\n",
      "Epoch 31/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.5882 - acc: 0.0405 - val_loss: 4.4595 - val_acc: 0.0646\n",
      "Epoch 32/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 4.5963 - acc: 0.0334 - val_loss: 4.4475 - val_acc: 0.0642\n",
      "Epoch 33/150\n",
      "28/28 [==============================] - 20s 717ms/step - loss: 4.5739 - acc: 0.0408 - val_loss: 4.4358 - val_acc: 0.0659\n",
      "Epoch 34/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.5771 - acc: 0.0386 - val_loss: 4.4236 - val_acc: 0.0698\n",
      "Epoch 35/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.5680 - acc: 0.0366 - val_loss: 4.4132 - val_acc: 0.0708\n",
      "Epoch 36/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5440 - acc: 0.0398 - val_loss: 4.4031 - val_acc: 0.0734\n",
      "Epoch 37/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.5443 - acc: 0.0384 - val_loss: 4.3913 - val_acc: 0.0756\n",
      "Epoch 38/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5365 - acc: 0.0410 - val_loss: 4.3821 - val_acc: 0.0753\n",
      "Epoch 39/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5363 - acc: 0.0420 - val_loss: 4.3690 - val_acc: 0.0789\n",
      "Epoch 40/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.5327 - acc: 0.0441 - val_loss: 4.3593 - val_acc: 0.0769\n",
      "Epoch 41/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5293 - acc: 0.0447 - val_loss: 4.3527 - val_acc: 0.0779\n",
      "Epoch 42/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.5137 - acc: 0.0460 - val_loss: 4.3429 - val_acc: 0.0779\n",
      "Epoch 43/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.5250 - acc: 0.0440 - val_loss: 4.3313 - val_acc: 0.0805\n",
      "Epoch 44/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.5105 - acc: 0.0458 - val_loss: 4.3236 - val_acc: 0.0844\n",
      "Epoch 45/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.5029 - acc: 0.0453 - val_loss: 4.3161 - val_acc: 0.0828\n",
      "Epoch 46/150\n",
      "28/28 [==============================] - 20s 714ms/step - loss: 4.4975 - acc: 0.0457 - val_loss: 4.3081 - val_acc: 0.0864\n",
      "Epoch 47/150\n",
      "28/28 [==============================] - 21s 738ms/step - loss: 4.4813 - acc: 0.0456 - val_loss: 4.2979 - val_acc: 0.0867\n",
      "Epoch 48/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4917 - acc: 0.0481 - val_loss: 4.2910 - val_acc: 0.0854\n",
      "Epoch 49/150\n",
      "28/28 [==============================] - 20s 705ms/step - loss: 4.4768 - acc: 0.0489 - val_loss: 4.2837 - val_acc: 0.0874\n",
      "Epoch 50/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4731 - acc: 0.0485 - val_loss: 4.2759 - val_acc: 0.0877\n",
      "Epoch 51/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.4511 - acc: 0.0544 - val_loss: 4.2690 - val_acc: 0.0867\n",
      "Epoch 52/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4733 - acc: 0.0495 - val_loss: 4.2599 - val_acc: 0.0854\n",
      "Epoch 53/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.4668 - acc: 0.0472 - val_loss: 4.2519 - val_acc: 0.0887\n",
      "Epoch 54/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4468 - acc: 0.0538 - val_loss: 4.2486 - val_acc: 0.0874\n",
      "Epoch 55/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4388 - acc: 0.0546 - val_loss: 4.2404 - val_acc: 0.0887\n",
      "Epoch 56/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.4462 - acc: 0.0546 - val_loss: 4.2362 - val_acc: 0.0913\n",
      "Epoch 57/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4529 - acc: 0.0537 - val_loss: 4.2264 - val_acc: 0.0926\n",
      "Epoch 58/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 4.4316 - acc: 0.0526 - val_loss: 4.2202 - val_acc: 0.0926\n",
      "Epoch 59/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 4.4458 - acc: 0.0548 - val_loss: 4.2171 - val_acc: 0.0913\n",
      "Epoch 60/150\n",
      "28/28 [==============================] - 20s 714ms/step - loss: 4.4532 - acc: 0.0560 - val_loss: 4.2122 - val_acc: 0.0919\n",
      "Epoch 61/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 4.4390 - acc: 0.0530 - val_loss: 4.2066 - val_acc: 0.0923\n",
      "Epoch 62/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4192 - acc: 0.0573 - val_loss: 4.2018 - val_acc: 0.0939\n",
      "Epoch 63/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4238 - acc: 0.0567 - val_loss: 4.1968 - val_acc: 0.0926\n",
      "Epoch 64/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.4151 - acc: 0.0520 - val_loss: 4.1909 - val_acc: 0.0942\n",
      "Epoch 65/150\n",
      "28/28 [==============================] - 20s 715ms/step - loss: 4.4237 - acc: 0.0550 - val_loss: 4.1837 - val_acc: 0.0949\n",
      "Epoch 66/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.4037 - acc: 0.0586 - val_loss: 4.1804 - val_acc: 0.0952\n",
      "Epoch 67/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.4222 - acc: 0.0548 - val_loss: 4.1752 - val_acc: 0.0981\n",
      "Epoch 68/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.4125 - acc: 0.0532 - val_loss: 4.1692 - val_acc: 0.0988\n",
      "Epoch 69/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3869 - acc: 0.0592 - val_loss: 4.1661 - val_acc: 0.0985\n",
      "Epoch 70/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.4103 - acc: 0.0583 - val_loss: 4.1604 - val_acc: 0.1014\n",
      "Epoch 71/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3993 - acc: 0.0592 - val_loss: 4.1545 - val_acc: 0.1011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3975 - acc: 0.0541 - val_loss: 4.1512 - val_acc: 0.1004\n",
      "Epoch 73/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.4041 - acc: 0.0598 - val_loss: 4.1498 - val_acc: 0.0988\n",
      "Epoch 74/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 4.3949 - acc: 0.0581 - val_loss: 4.1466 - val_acc: 0.1040\n",
      "Epoch 75/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3735 - acc: 0.0601 - val_loss: 4.1402 - val_acc: 0.1047\n",
      "Epoch 76/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3811 - acc: 0.0630 - val_loss: 4.1336 - val_acc: 0.1040\n",
      "Epoch 77/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.3761 - acc: 0.0598 - val_loss: 4.1318 - val_acc: 0.1030\n",
      "Epoch 78/150\n",
      "28/28 [==============================] - 20s 707ms/step - loss: 4.3751 - acc: 0.0592 - val_loss: 4.1270 - val_acc: 0.1040\n",
      "Epoch 79/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3715 - acc: 0.0588 - val_loss: 4.1227 - val_acc: 0.1047\n",
      "Epoch 80/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3676 - acc: 0.0633 - val_loss: 4.1206 - val_acc: 0.1053\n",
      "Epoch 81/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.3731 - acc: 0.0616 - val_loss: 4.1174 - val_acc: 0.1076\n",
      "Epoch 82/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3538 - acc: 0.0626 - val_loss: 4.1167 - val_acc: 0.1066\n",
      "Epoch 83/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.3595 - acc: 0.0611 - val_loss: 4.1135 - val_acc: 0.1069\n",
      "Epoch 84/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3593 - acc: 0.0619 - val_loss: 4.1090 - val_acc: 0.1069\n",
      "Epoch 85/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3697 - acc: 0.0598 - val_loss: 4.1072 - val_acc: 0.1089\n",
      "Epoch 86/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3707 - acc: 0.0654 - val_loss: 4.1034 - val_acc: 0.1076\n",
      "Epoch 87/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3630 - acc: 0.0608 - val_loss: 4.0986 - val_acc: 0.1066\n",
      "Epoch 88/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3627 - acc: 0.0638 - val_loss: 4.0966 - val_acc: 0.1069\n",
      "Epoch 89/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3607 - acc: 0.0614 - val_loss: 4.0939 - val_acc: 0.1082\n",
      "Epoch 90/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.3408 - acc: 0.0653 - val_loss: 4.0897 - val_acc: 0.1063\n",
      "Epoch 91/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3478 - acc: 0.0621 - val_loss: 4.0882 - val_acc: 0.1082\n",
      "Epoch 92/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.3510 - acc: 0.0677 - val_loss: 4.0851 - val_acc: 0.1099\n",
      "Epoch 93/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3368 - acc: 0.0706 - val_loss: 4.0817 - val_acc: 0.1105\n",
      "Epoch 94/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3464 - acc: 0.0630 - val_loss: 4.0779 - val_acc: 0.1099\n",
      "Epoch 95/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3489 - acc: 0.0637 - val_loss: 4.0765 - val_acc: 0.1099\n",
      "Epoch 96/150\n",
      "28/28 [==============================] - 20s 717ms/step - loss: 4.3320 - acc: 0.0677 - val_loss: 4.0753 - val_acc: 0.1109\n",
      "Epoch 97/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3634 - acc: 0.0608 - val_loss: 4.0713 - val_acc: 0.1096\n",
      "Epoch 98/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3515 - acc: 0.0619 - val_loss: 4.0699 - val_acc: 0.1102\n",
      "Epoch 99/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3413 - acc: 0.0662 - val_loss: 4.0654 - val_acc: 0.1099\n",
      "Epoch 100/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3526 - acc: 0.0633 - val_loss: 4.0641 - val_acc: 0.1086\n",
      "Epoch 101/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3364 - acc: 0.0675 - val_loss: 4.0632 - val_acc: 0.1122\n",
      "Epoch 102/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 4.3236 - acc: 0.0681 - val_loss: 4.0594 - val_acc: 0.1105\n",
      "Epoch 103/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.3259 - acc: 0.0603 - val_loss: 4.0584 - val_acc: 0.1096\n",
      "Epoch 104/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3143 - acc: 0.0675 - val_loss: 4.0557 - val_acc: 0.1099\n",
      "Epoch 105/150\n",
      "28/28 [==============================] - 20s 707ms/step - loss: 4.3294 - acc: 0.0688 - val_loss: 4.0532 - val_acc: 0.1112\n",
      "Epoch 106/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3380 - acc: 0.0674 - val_loss: 4.0522 - val_acc: 0.1105\n",
      "Epoch 107/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3205 - acc: 0.0696 - val_loss: 4.0498 - val_acc: 0.1122\n",
      "Epoch 108/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3058 - acc: 0.0687 - val_loss: 4.0478 - val_acc: 0.1105\n",
      "Epoch 109/150\n",
      "28/28 [==============================] - 20s 707ms/step - loss: 4.3156 - acc: 0.0669 - val_loss: 4.0448 - val_acc: 0.1131\n",
      "Epoch 110/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.3038 - acc: 0.0715 - val_loss: 4.0415 - val_acc: 0.1112\n",
      "Epoch 111/150\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 4.3414 - acc: 0.0666 - val_loss: 4.0404 - val_acc: 0.1112\n",
      "Epoch 112/150\n",
      "28/28 [==============================] - 20s 707ms/step - loss: 4.3127 - acc: 0.0653 - val_loss: 4.0371 - val_acc: 0.1118\n",
      "Epoch 113/150\n",
      "28/28 [==============================] - 20s 712ms/step - loss: 4.3306 - acc: 0.0685 - val_loss: 4.0358 - val_acc: 0.1115\n",
      "Epoch 114/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3129 - acc: 0.0685 - val_loss: 4.0353 - val_acc: 0.1109\n",
      "Epoch 115/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3130 - acc: 0.0675 - val_loss: 4.0337 - val_acc: 0.1115\n",
      "Epoch 116/150\n",
      "28/28 [==============================] - 20s 709ms/step - loss: 4.3051 - acc: 0.0668 - val_loss: 4.0325 - val_acc: 0.1105\n",
      "Epoch 117/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3208 - acc: 0.0677 - val_loss: 4.0297 - val_acc: 0.1099\n",
      "Epoch 118/150\n",
      "28/28 [==============================] - 20s 716ms/step - loss: 4.3157 - acc: 0.0657 - val_loss: 4.0252 - val_acc: 0.1125\n",
      "Epoch 119/150\n",
      "28/28 [==============================] - 20s 710ms/step - loss: 4.3209 - acc: 0.0684 - val_loss: 4.0250 - val_acc: 0.1122\n",
      "Epoch 00119: early stopping\n",
      "Test accuracy: 0.112161721557\n",
      "Creating model\n",
      "\n",
      "dropout= 0.7919157826643053\n",
      "layers= 0\n",
      "dontFreeze= 3\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 23s 825ms/step - loss: 5.0244 - acc: 0.0092 - val_loss: 4.7874 - val_acc: 0.0065\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 4.7877 - acc: 0.0109 - val_loss: 4.7873 - val_acc: 0.0137\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 21s 748ms/step - loss: 4.7872 - acc: 0.0097 - val_loss: 4.7872 - val_acc: 0.0091\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 21s 746ms/step - loss: 4.7870 - acc: 0.0116 - val_loss: 4.7871 - val_acc: 0.0137\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 4.7868 - acc: 0.0117 - val_loss: 4.7870 - val_acc: 0.0137\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 21s 747ms/step - loss: 4.7866 - acc: 0.0117 - val_loss: 4.7869 - val_acc: 0.0137\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 21s 745ms/step - loss: 4.7865 - acc: 0.0117 - val_loss: 4.7868 - val_acc: 0.0137\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 4.7864 - acc: 0.0117 - val_loss: 4.7867 - val_acc: 0.0137\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 4.7862 - acc: 0.0117 - val_loss: 4.7866 - val_acc: 0.0137\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 21s 751ms/step - loss: 4.7861 - acc: 0.0117 - val_loss: 4.7865 - val_acc: 0.0137\n",
      "Epoch 11/150\n",
      "28/28 [==============================] - 21s 750ms/step - loss: 4.7859 - acc: 0.0117 - val_loss: 4.7865 - val_acc: 0.0137\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 4.7858 - acc: 0.0117 - val_loss: 4.7864 - val_acc: 0.0137\n",
      "Epoch 00012: early stopping\n",
      "Test accuracy: 0.0136941636779\n",
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:39<00:00, 259.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training/validation\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-27baf91117af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           notebook_name='VGG16Optimization_1')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dddbeb5d4816>\u001b[0m in \u001b[0;36mdata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Splitting into training/validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv_project/lib/python3.5/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='VGG16Optimization_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-864038290e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "val_loss, val_acc = best_model.evaluate(X_test, Y_test);\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(\"Validation loss: \", val_loss)\n",
    "print(\"Validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(modelPath);\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, accuracy is low here because we are not taking advantage of the pre-trained weights as they cannot be downloaded in the kernel. This means we are training the wights from scratch and I we have only run 1 epoch due to the hardware constraints in the kernel.\n",
    "\n",
    "Next we will make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.DataFrame(preds)\n",
    "# # Set column names to those generated by the one-hot encoding earlier\n",
    "# col_names = one_hot.columns.values\n",
    "# sub.columns = col_names\n",
    "# # Insert the column id from the sample_submission at the start of the data frame\n",
    "# sub.insert(0, 'id', df_test['id'])\n",
    "# sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
