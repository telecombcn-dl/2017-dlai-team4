{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv\n",
      "sample_submission.csv\n",
      "test\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications import Xception\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# parameters #\n",
    "##############\n",
    "# dontFreezeLast = 0;\n",
    "\n",
    "# patience = 10;\n",
    "\n",
    "# gpuName = '/device:GPU:0'\n",
    "# workers = 2;\n",
    "# histogram_freq = 0;\n",
    "\n",
    "# epochs = 100;\n",
    "# validation_size=0.3;\n",
    "\n",
    "modelPath = '../models/Xception_opt/';\n",
    "modelName = 'run1.h5';\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read in the csv's so we can see some more information on the filenames and breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../input/labels.csv')\n",
    "# df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "# print('Training images: ',df_train.shape[0])\n",
    "# print('Test images: ',df_test.shape[0])\n",
    "\n",
    "# reduce dimensionality\n",
    "#df_train = df_train.head(100)\n",
    "#df_test = df_test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the breed needs to be one-hot encoded for the final submission, so we will now do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_series = pd.Series(df_train['breed'])\n",
    "# one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will read in all of the images for test and train, using a for loop through the values of the csv files. I have also set an im_size variable which sets the size for the image to be re-sized to, 90x90 px, you should play with this number to see how it affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0 \n",
    "# for f, breed in tqdm(df_train.values[:10]):\n",
    "#     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "#     label = one_hot_labels[i]\n",
    "#     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "#     y_train.append(label)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in tqdm(df_test['id'].values):\n",
    "#     img = cv2.imread('../input/test/{}.jpg'.format(f))\n",
    "#     x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_raw = np.array(y_train, np.uint8)\n",
    "# x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "# x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of the outputs to make sure everyting went as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_raw.shape)\n",
    "# print(y_train_raw.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that there are 120 different breeds. We can put this in a num_class variable below that can then be used when creating the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = y_train_raw.shape[1]\n",
    "# print('Number of classes: ', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to create a validation set so that you can gauge the performance of your model on independent data, unseen to the model in training. We do this by splitting the current training set (x_train_raw) and the corresponding labels (y_train_raw) so that we set aside 30 % of the data at random and put these in validation sets (X_valid and Y_valid).\n",
    "\n",
    "* This split needs to be improved so that it contains images from every class, with 120 separate classes some can not be represented and so the validation score is not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=validation_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the CNN architecture. Here we are using a pre-trained model VGG19 which has already been trained to identify many different dog breeds (as well as a lot of other objects from the imagenet dataset see here for more information: http://image-net.org/about-overview). Unfortunately it doesn't seem possible to downlod the weights from within this kernel so make sure you set the weights argument to 'imagenet' and not None, as it currently is below.\n",
    "\n",
    "We then remove the final layer and instead replace it with a single dense layer with the number of nodes corresponding to the number of breed classes we have (120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    print('Getting data')\n",
    "    df_train = pd.read_csv('../input/labels.csv')\n",
    "    df_test = pd.read_csv('../input/sample_submission.csv')\n",
    "    \n",
    "    targets_series = pd.Series(df_train['breed'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "    \n",
    "    im_size = 90\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    \n",
    "    i = 0 \n",
    "    for f, breed in tqdm(df_train.values[:8000]):\n",
    "        img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "    \n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    num_class = y_train_raw.shape[1]\n",
    "    \n",
    "    print('Splitting into training/validation')\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
    "    \n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and model for hyperas\n",
    "\n",
    "def model(X_train,Y_train,X_valid,Y_valid):\n",
    "    print('Creating model')\n",
    "    base_model = Xception(weights = 'imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(im_size, im_size, 3))\n",
    "\n",
    "    dropout = {{uniform(0.5,1)}};\n",
    "    layers = {{choice([0,1,2])}};\n",
    "    dontFreeze = {{choice(list(range(5+1)))}};\n",
    "    batchSize = {{choice([16,64,256])}};\n",
    "    \n",
    "    print()\n",
    "    print('dropout=',dropout)\n",
    "    print('layers=',layers)\n",
    "    print('dontFreeze=',dontFreeze)\n",
    "    print('batchSize=',batchSize)\n",
    "    print()\n",
    "    \n",
    "    stepsPerEpoch = round( len(X_train)/batchSize );\n",
    "    \n",
    "    # Add a new top layer\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    if layers>=2:\n",
    "        x = Dense(1024,activation='relu')(x)\n",
    "    if layers>=1:\n",
    "        x = Dense(512,activation='relu')(x)\n",
    "    # in any case:\n",
    "    predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "    # This is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # First: train only the top layers (which were randomly initialized)\n",
    "    for i in range(len(base_model.layers)-dontFreeze):\n",
    "        base_model.layers[i].trainable = False\n",
    "\n",
    "    # predetermined optimizer\n",
    "    lr=0.00020389590556056983;\n",
    "    beta_1=0.9453158868247398;\n",
    "    beta_2=0.9925872692991417;\n",
    "    decay=0.000821336141287018;\n",
    "    adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=adam, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks_list = [];\n",
    "    callbacks_list.append(keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=1));\n",
    "\n",
    "\n",
    "    # data augmentation & fitting\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.5,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True);\n",
    "    \n",
    "    model.fit_generator(\n",
    "        datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
    "        steps_per_epoch=stepsPerEpoch,\n",
    "        epochs=150,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid,Y_valid),\n",
    "        workers=2,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks_list)\n",
    "#     model.fit(X_train, Y_train,\n",
    "#       epochs=100,\n",
    "#       batch_size = batchSize,\n",
    "#       validation_data=(X_valid, Y_valid),\n",
    "#       verbose=1,\n",
    "#       callbacks=callbacks_list)\n",
    "\n",
    "    score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications import Xception\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from subprocess import check_output\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout': hp.uniform('dropout', 0.5,1),\n",
      "        'layers': hp.choice('layers', [0,1,2]),\n",
      "        'dontFreeze': hp.choice('dontFreeze', list(range(5+1))),\n",
      "        'batchSize': hp.choice('batchSize', [16,64,256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: print('Getting data')\n",
      "  3: df_train = pd.read_csv('../input/labels.csv')\n",
      "  4: df_test = pd.read_csv('../input/sample_submission.csv')\n",
      "  5: \n",
      "  6: targets_series = pd.Series(df_train['breed'])\n",
      "  7: one_hot = pd.get_dummies(targets_series, sparse = True)\n",
      "  8: one_hot_labels = np.asarray(one_hot)\n",
      "  9: \n",
      " 10: im_size = 90\n",
      " 11: x_train = []\n",
      " 12: y_train = []\n",
      " 13: x_test = []\n",
      " 14: \n",
      " 15: i = 0 \n",
      " 16: for f, breed in tqdm(df_train.values):\n",
      " 17:     img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
      " 18:     label = one_hot_labels[i]\n",
      " 19:     x_train.append(cv2.resize(img, (im_size, im_size)))\n",
      " 20:     y_train.append(label)\n",
      " 21:     i += 1\n",
      " 22: \n",
      " 23: y_train_raw = np.array(y_train, np.uint8)\n",
      " 24: x_train_raw = np.array(x_train, np.float32) / 255.\n",
      " 25: num_class = y_train_raw.shape[1]\n",
      " 26: \n",
      " 27: print('Splitting into training/validation')\n",
      " 28: X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)\n",
      " 29: \n",
      " 30: \n",
      " 31: \n",
      " 32: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     print('Creating model')\n",
      "   4:     base_model = Xception(weights = 'imagenet',\n",
      "   5:                        include_top=False,\n",
      "   6:                        input_shape=(im_size, im_size, 3))\n",
      "   7: \n",
      "   8:     dropout = space['dropout'];\n",
      "   9:     layers = space['layers'];\n",
      "  10:     dontFreeze = space['dontFreeze'];\n",
      "  11:     batchSize = space['batchSize'];\n",
      "  12:     \n",
      "  13:     print()\n",
      "  14:     print('dropout=',dropout)\n",
      "  15:     print('layers=',layers)\n",
      "  16:     print('dontFreeze=',dontFreeze)\n",
      "  17:     print('batchSize=',batchSize)\n",
      "  18:     print()\n",
      "  19:     \n",
      "  20:     stepsPerEpoch = round( len(X_train)/batchSize );\n",
      "  21:     \n",
      "  22:     # Add a new top layer\n",
      "  23:     x = base_model.output\n",
      "  24:     x = Flatten()(x)\n",
      "  25:     x = Dropout(dropout)(x)\n",
      "  26:     if layers>=2:\n",
      "  27:         x = Dense(1024,activation='relu')(x)\n",
      "  28:     if layers>=1:\n",
      "  29:         x = Dense(512,activation='relu')(x)\n",
      "  30:     # in any case:\n",
      "  31:     predictions = Dense(num_class, activation='softmax')(x)\n",
      "  32: \n",
      "  33:     # This is the model we will train\n",
      "  34:     model = Model(inputs=base_model.input, outputs=predictions)\n",
      "  35: \n",
      "  36:     # First: train only the top layers (which were randomly initialized)\n",
      "  37:     for i in range(len(base_model.layers)-dontFreeze):\n",
      "  38:         base_model.layers[i].trainable = False\n",
      "  39: \n",
      "  40:     # predetermined optimizer\n",
      "  41:     lr=0.00020389590556056983;\n",
      "  42:     beta_1=0.9453158868247398;\n",
      "  43:     beta_2=0.9925872692991417;\n",
      "  44:     decay=0.000821336141287018;\n",
      "  45:     adam = keras.optimizers.Adam(lr=lr,beta_1=beta_1,beta_2=beta_2,decay=decay)\n",
      "  46:     model.compile(loss='categorical_crossentropy', \n",
      "  47:                   optimizer=adam, \n",
      "  48:                   metrics=['accuracy'])\n",
      "  49: \n",
      "  50:     callbacks_list = [];\n",
      "  51:     callbacks_list.append(keras.callbacks.EarlyStopping(\n",
      "  52:         monitor='val_acc',\n",
      "  53:         patience=10,\n",
      "  54:         verbose=1));\n",
      "  55: \n",
      "  56: \n",
      "  57:     # data augmentation & fitting\n",
      "  58:     datagen = ImageDataGenerator(\n",
      "  59:         rotation_range=30,\n",
      "  60:         width_shift_range=0.1,\n",
      "  61:         height_shift_range=0.1,\n",
      "  62:         shear_range=0.5,\n",
      "  63:         zoom_range=0.5,\n",
      "  64:         horizontal_flip=True,\n",
      "  65:         vertical_flip=True);\n",
      "  66:     \n",
      "  67:     model.fit_generator(\n",
      "  68:         datagen.flow(X_train,Y_train,batch_size=batchSize),\n",
      "  69:         steps_per_epoch=stepsPerEpoch,\n",
      "  70:         epochs=150,\n",
      "  71:         verbose=1,\n",
      "  72:         validation_data=(X_valid,Y_valid),\n",
      "  73:         workers=2,\n",
      "  74:         shuffle=True,\n",
      "  75:         callbacks=callbacks_list)\n",
      "  76: #     model.fit(X_train, Y_train,\n",
      "  77: #       epochs=100,\n",
      "  78: #       batch_size = batchSize,\n",
      "  79: #       validation_data=(X_valid, Y_valid),\n",
      "  80: #       verbose=1,\n",
      "  81: #       callbacks=callbacks_list)\n",
      "  82: \n",
      "  83:     score, acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
      "  84:     print('Test accuracy:', acc)\n",
      "  85:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  86: \n",
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:29<00:00, 344.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into training/validation\n",
      "Creating model\n",
      "\n",
      "dropout= 0.6064002165637792\n",
      "layers= 2\n",
      "dontFreeze= 3\n",
      "batchSize= 64\n",
      "\n",
      "Epoch 1/150\n",
      "112/112 [==============================] - 35s 316ms/step - loss: 4.8225 - acc: 0.0126 - val_loss: 4.7719 - val_acc: 0.0163\n",
      "Epoch 2/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 4.7440 - acc: 0.0228 - val_loss: 4.6149 - val_acc: 0.0496\n",
      "Epoch 3/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 4.6401 - acc: 0.0364 - val_loss: 4.4221 - val_acc: 0.0623\n",
      "Epoch 4/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 4.5119 - acc: 0.0460 - val_loss: 4.2803 - val_acc: 0.0747\n",
      "Epoch 5/150\n",
      "112/112 [==============================] - 24s 215ms/step - loss: 4.3910 - acc: 0.0672 - val_loss: 4.1704 - val_acc: 0.0841\n",
      "Epoch 6/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.2991 - acc: 0.0710 - val_loss: 4.1244 - val_acc: 0.0877\n",
      "Epoch 7/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.2248 - acc: 0.0768 - val_loss: 4.0232 - val_acc: 0.0939\n",
      "Epoch 8/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.1738 - acc: 0.0820 - val_loss: 3.9678 - val_acc: 0.1151\n",
      "Epoch 9/150\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 4.1054 - acc: 0.0920 - val_loss: 3.9406 - val_acc: 0.1089\n",
      "Epoch 10/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.0680 - acc: 0.1000 - val_loss: 3.9446 - val_acc: 0.1092\n",
      "Epoch 11/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 4.0111 - acc: 0.1090 - val_loss: 3.8914 - val_acc: 0.1246\n",
      "Epoch 12/150\n",
      "112/112 [==============================] - 24s 215ms/step - loss: 4.0041 - acc: 0.1020 - val_loss: 3.8782 - val_acc: 0.1210\n",
      "Epoch 13/150\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 3.9637 - acc: 0.1164 - val_loss: 3.8503 - val_acc: 0.1278\n",
      "Epoch 14/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 3.9395 - acc: 0.1181 - val_loss: 3.8407 - val_acc: 0.1285\n",
      "Epoch 15/150\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 3.9138 - acc: 0.1205 - val_loss: 3.8169 - val_acc: 0.1298\n",
      "Epoch 16/150\n",
      "112/112 [==============================] - 24s 215ms/step - loss: 3.8857 - acc: 0.1207 - val_loss: 3.7902 - val_acc: 0.1301\n",
      "Epoch 17/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.8641 - acc: 0.1269 - val_loss: 3.7802 - val_acc: 0.1412\n",
      "Epoch 18/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.8614 - acc: 0.1209 - val_loss: 3.7871 - val_acc: 0.1350\n",
      "Epoch 19/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.8415 - acc: 0.1272 - val_loss: 3.8058 - val_acc: 0.1360\n",
      "Epoch 20/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.8225 - acc: 0.1278 - val_loss: 3.7612 - val_acc: 0.1389\n",
      "Epoch 21/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.7739 - acc: 0.1379 - val_loss: 3.7564 - val_acc: 0.1350\n",
      "Epoch 22/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.7855 - acc: 0.1368 - val_loss: 3.7534 - val_acc: 0.1382\n",
      "Epoch 23/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.7836 - acc: 0.1350 - val_loss: 3.7527 - val_acc: 0.1337\n",
      "Epoch 24/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.7627 - acc: 0.1395 - val_loss: 3.7337 - val_acc: 0.1487\n",
      "Epoch 25/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.7355 - acc: 0.1420 - val_loss: 3.7286 - val_acc: 0.1422\n",
      "Epoch 26/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.7432 - acc: 0.1480 - val_loss: 3.7502 - val_acc: 0.1402\n",
      "Epoch 27/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.7393 - acc: 0.1462 - val_loss: 3.7413 - val_acc: 0.1405\n",
      "Epoch 28/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.7084 - acc: 0.1442 - val_loss: 3.7297 - val_acc: 0.1467\n",
      "Epoch 29/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.6715 - acc: 0.1568 - val_loss: 3.7244 - val_acc: 0.1431\n",
      "Epoch 30/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.6811 - acc: 0.1585 - val_loss: 3.7275 - val_acc: 0.1448\n",
      "Epoch 31/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.6733 - acc: 0.1580 - val_loss: 3.7036 - val_acc: 0.1461\n",
      "Epoch 32/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.6688 - acc: 0.1551 - val_loss: 3.7306 - val_acc: 0.1451\n",
      "Epoch 33/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.6487 - acc: 0.1573 - val_loss: 3.7009 - val_acc: 0.1487\n",
      "Epoch 34/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.6365 - acc: 0.1640 - val_loss: 3.7059 - val_acc: 0.1497\n",
      "Epoch 35/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.6565 - acc: 0.1625 - val_loss: 3.6856 - val_acc: 0.1516\n",
      "Epoch 36/150\n",
      "112/112 [==============================] - 24s 214ms/step - loss: 3.6042 - acc: 0.1698 - val_loss: 3.6931 - val_acc: 0.1519\n",
      "Epoch 37/150\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 3.6115 - acc: 0.1619 - val_loss: 3.6792 - val_acc: 0.1604\n",
      "Epoch 38/150\n",
      "112/112 [==============================] - 24s 217ms/step - loss: 3.5919 - acc: 0.1714 - val_loss: 3.6924 - val_acc: 0.1620\n",
      "Epoch 39/150\n",
      "112/112 [==============================] - 24s 216ms/step - loss: 3.6097 - acc: 0.1687 - val_loss: 3.7006 - val_acc: 0.1559\n",
      "Epoch 40/150\n",
      "112/112 [==============================] - 24s 215ms/step - loss: 3.6144 - acc: 0.1638 - val_loss: 3.6875 - val_acc: 0.1572\n",
      "Epoch 41/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5866 - acc: 0.1714 - val_loss: 3.6875 - val_acc: 0.1497\n",
      "Epoch 42/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5734 - acc: 0.1737 - val_loss: 3.6897 - val_acc: 0.1513\n",
      "Epoch 43/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5774 - acc: 0.1743 - val_loss: 3.6737 - val_acc: 0.1549\n",
      "Epoch 44/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5539 - acc: 0.1783 - val_loss: 3.6875 - val_acc: 0.1552\n",
      "Epoch 45/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5464 - acc: 0.1687 - val_loss: 3.6778 - val_acc: 0.1559\n",
      "Epoch 46/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5029 - acc: 0.1835 - val_loss: 3.6723 - val_acc: 0.1526\n",
      "Epoch 47/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5294 - acc: 0.1810 - val_loss: 3.6927 - val_acc: 0.1545\n",
      "Epoch 48/150\n",
      "112/112 [==============================] - 24s 213ms/step - loss: 3.5446 - acc: 0.1754 - val_loss: 3.6640 - val_acc: 0.1594\n",
      "Epoch 00048: early stopping\n",
      "Test accuracy: 0.1594391914\n",
      "Creating model\n",
      "\n",
      "dropout= 0.6165806722373868\n",
      "layers= 0\n",
      "dontFreeze= 4\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 22s 774ms/step - loss: 4.9565 - acc: 0.0109 - val_loss: 5.0031 - val_acc: 0.0326\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.7946 - acc: 0.0203 - val_loss: 4.5057 - val_acc: 0.0606\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 4.6708 - acc: 0.0314 - val_loss: 4.3756 - val_acc: 0.0890\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 4.5994 - acc: 0.0448 - val_loss: 4.2235 - val_acc: 0.1024\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 19s 676ms/step - loss: 4.5363 - acc: 0.0497 - val_loss: 4.1550 - val_acc: 0.1102\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.4719 - acc: 0.0591 - val_loss: 4.0846 - val_acc: 0.1096\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 4.4114 - acc: 0.0703 - val_loss: 4.0300 - val_acc: 0.1187\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 4.3561 - acc: 0.0791 - val_loss: 4.0071 - val_acc: 0.1184\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.3165 - acc: 0.0784 - val_loss: 3.9458 - val_acc: 0.1223\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.2431 - acc: 0.0935 - val_loss: 3.9500 - val_acc: 0.1239\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 19s 678ms/step - loss: 4.2225 - acc: 0.0870 - val_loss: 3.8909 - val_acc: 0.1249\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.1965 - acc: 0.0984 - val_loss: 3.8830 - val_acc: 0.1278\n",
      "Epoch 13/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 4.1349 - acc: 0.1009 - val_loss: 3.8901 - val_acc: 0.1288\n",
      "Epoch 14/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.1018 - acc: 0.1066 - val_loss: 3.8659 - val_acc: 0.1307\n",
      "Epoch 15/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.0672 - acc: 0.1105 - val_loss: 3.8585 - val_acc: 0.1304\n",
      "Epoch 16/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.0493 - acc: 0.1119 - val_loss: 3.8428 - val_acc: 0.1350\n",
      "Epoch 17/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.0224 - acc: 0.1167 - val_loss: 3.8412 - val_acc: 0.1340\n",
      "Epoch 18/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 4.0004 - acc: 0.1233 - val_loss: 3.8308 - val_acc: 0.1428\n",
      "Epoch 19/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.9568 - acc: 0.1234 - val_loss: 3.8085 - val_acc: 0.1428\n",
      "Epoch 20/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.9425 - acc: 0.1233 - val_loss: 3.8149 - val_acc: 0.1396\n",
      "Epoch 21/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.9424 - acc: 0.1270 - val_loss: 3.8070 - val_acc: 0.1386\n",
      "Epoch 22/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.9032 - acc: 0.1281 - val_loss: 3.7878 - val_acc: 0.1405\n",
      "Epoch 23/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.8873 - acc: 0.1325 - val_loss: 3.7953 - val_acc: 0.1422\n",
      "Epoch 24/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 3.8926 - acc: 0.1274 - val_loss: 3.7881 - val_acc: 0.1428\n",
      "Epoch 25/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 3.8521 - acc: 0.1399 - val_loss: 3.7769 - val_acc: 0.1457\n",
      "Epoch 26/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 3.8604 - acc: 0.1345 - val_loss: 3.7808 - val_acc: 0.1402\n",
      "Epoch 27/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.8112 - acc: 0.1475 - val_loss: 3.7668 - val_acc: 0.1405\n",
      "Epoch 28/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.7897 - acc: 0.1463 - val_loss: 3.7620 - val_acc: 0.1493\n",
      "Epoch 29/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.8044 - acc: 0.1456 - val_loss: 3.7658 - val_acc: 0.1467\n",
      "Epoch 30/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.7846 - acc: 0.1466 - val_loss: 3.7649 - val_acc: 0.1454\n",
      "Epoch 31/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 3.7740 - acc: 0.1515 - val_loss: 3.7592 - val_acc: 0.1484\n",
      "Epoch 32/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.7691 - acc: 0.1474 - val_loss: 3.7552 - val_acc: 0.1510\n",
      "Epoch 33/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.7881 - acc: 0.1420 - val_loss: 3.7486 - val_acc: 0.1464\n",
      "Epoch 34/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.7447 - acc: 0.1450 - val_loss: 3.7488 - val_acc: 0.1506\n",
      "Epoch 35/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.7329 - acc: 0.1471 - val_loss: 3.7414 - val_acc: 0.1513\n",
      "Epoch 36/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 3.7159 - acc: 0.1622 - val_loss: 3.7449 - val_acc: 0.1519\n",
      "Epoch 37/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.7409 - acc: 0.1475 - val_loss: 3.7350 - val_acc: 0.1497\n",
      "Epoch 38/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.7095 - acc: 0.1561 - val_loss: 3.7412 - val_acc: 0.1523\n",
      "Epoch 39/150\n",
      "28/28 [==============================] - 19s 686ms/step - loss: 3.6900 - acc: 0.1554 - val_loss: 3.7339 - val_acc: 0.1555\n",
      "Epoch 40/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 3.6740 - acc: 0.1615 - val_loss: 3.7307 - val_acc: 0.1578\n",
      "Epoch 41/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.6723 - acc: 0.1607 - val_loss: 3.7369 - val_acc: 0.1549\n",
      "Epoch 42/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.6570 - acc: 0.1650 - val_loss: 3.7342 - val_acc: 0.1575\n",
      "Epoch 43/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 3.6439 - acc: 0.1617 - val_loss: 3.7366 - val_acc: 0.1532\n",
      "Epoch 44/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.6229 - acc: 0.1694 - val_loss: 3.7307 - val_acc: 0.1513\n",
      "Epoch 45/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 3.6266 - acc: 0.1621 - val_loss: 3.7297 - val_acc: 0.1536\n",
      "Epoch 46/150\n",
      "28/28 [==============================] - 19s 678ms/step - loss: 3.6462 - acc: 0.1580 - val_loss: 3.7244 - val_acc: 0.1526\n",
      "Epoch 47/150\n",
      "28/28 [==============================] - 19s 677ms/step - loss: 3.6287 - acc: 0.1651 - val_loss: 3.7186 - val_acc: 0.1536\n",
      "Epoch 48/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 3.6059 - acc: 0.1741 - val_loss: 3.7187 - val_acc: 0.1572\n",
      "Epoch 49/150\n",
      "28/28 [==============================] - 19s 683ms/step - loss: 3.6186 - acc: 0.1780 - val_loss: 3.7157 - val_acc: 0.1536\n",
      "Epoch 50/150\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 3.6130 - acc: 0.1692 - val_loss: 3.7172 - val_acc: 0.1549\n",
      "Epoch 00050: early stopping\n",
      "Test accuracy: 0.154874470176\n",
      "Creating model\n",
      "\n",
      "dropout= 0.8814062151635532\n",
      "layers= 1\n",
      "dontFreeze= 4\n",
      "batchSize= 64\n",
      "\n",
      "Epoch 1/150\n",
      "112/112 [==============================] - 26s 228ms/step - loss: 4.9485 - acc: 0.0103 - val_loss: 4.7766 - val_acc: 0.0183\n",
      "Epoch 2/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.8053 - acc: 0.0130 - val_loss: 4.7616 - val_acc: 0.0134\n",
      "Epoch 3/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.7789 - acc: 0.0177 - val_loss: 4.7456 - val_acc: 0.0284\n",
      "Epoch 4/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.7612 - acc: 0.0151 - val_loss: 4.7118 - val_acc: 0.0277\n",
      "Epoch 5/150\n",
      "112/112 [==============================] - 22s 199ms/step - loss: 4.7323 - acc: 0.0183 - val_loss: 4.6651 - val_acc: 0.0401\n",
      "Epoch 6/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.7031 - acc: 0.0254 - val_loss: 4.6034 - val_acc: 0.0440\n",
      "Epoch 7/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.6670 - acc: 0.0295 - val_loss: 4.5267 - val_acc: 0.0515\n",
      "Epoch 8/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.6333 - acc: 0.0317 - val_loss: 4.4600 - val_acc: 0.0522\n",
      "Epoch 9/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.5928 - acc: 0.0351 - val_loss: 4.4019 - val_acc: 0.0561\n",
      "Epoch 10/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.5477 - acc: 0.0433 - val_loss: 4.3463 - val_acc: 0.0659\n",
      "Epoch 11/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.5255 - acc: 0.0468 - val_loss: 4.2899 - val_acc: 0.0730\n",
      "Epoch 12/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.5005 - acc: 0.0477 - val_loss: 4.2498 - val_acc: 0.0766\n",
      "Epoch 13/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.4549 - acc: 0.0544 - val_loss: 4.2096 - val_acc: 0.0841\n",
      "Epoch 14/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.4396 - acc: 0.0520 - val_loss: 4.1866 - val_acc: 0.0880\n",
      "Epoch 15/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.4183 - acc: 0.0571 - val_loss: 4.1598 - val_acc: 0.0861\n",
      "Epoch 16/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.3908 - acc: 0.0560 - val_loss: 4.1357 - val_acc: 0.0916\n",
      "Epoch 17/150\n",
      "112/112 [==============================] - 22s 199ms/step - loss: 4.3751 - acc: 0.0576 - val_loss: 4.1086 - val_acc: 0.0946\n",
      "Epoch 18/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.3465 - acc: 0.0614 - val_loss: 4.1037 - val_acc: 0.0939\n",
      "Epoch 19/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.3402 - acc: 0.0657 - val_loss: 4.0750 - val_acc: 0.0929\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 22s 200ms/step - loss: 4.3056 - acc: 0.0666 - val_loss: 4.0565 - val_acc: 0.1007\n",
      "Epoch 21/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.2990 - acc: 0.0690 - val_loss: 4.0434 - val_acc: 0.1007\n",
      "Epoch 22/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.2960 - acc: 0.0769 - val_loss: 4.0404 - val_acc: 0.0994\n",
      "Epoch 23/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.2837 - acc: 0.0753 - val_loss: 4.0330 - val_acc: 0.1014\n",
      "Epoch 24/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.2701 - acc: 0.0763 - val_loss: 4.0140 - val_acc: 0.1017\n",
      "Epoch 25/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.2646 - acc: 0.0803 - val_loss: 4.0007 - val_acc: 0.1076\n",
      "Epoch 26/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.2425 - acc: 0.0730 - val_loss: 3.9807 - val_acc: 0.1082\n",
      "Epoch 27/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.2232 - acc: 0.0812 - val_loss: 3.9755 - val_acc: 0.1089\n",
      "Epoch 28/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.2264 - acc: 0.0761 - val_loss: 3.9594 - val_acc: 0.1154\n",
      "Epoch 29/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.1978 - acc: 0.0817 - val_loss: 3.9486 - val_acc: 0.1164\n",
      "Epoch 30/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.2049 - acc: 0.0812 - val_loss: 3.9419 - val_acc: 0.1190\n",
      "Epoch 31/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.1815 - acc: 0.0819 - val_loss: 3.9323 - val_acc: 0.1200\n",
      "Epoch 32/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1737 - acc: 0.0810 - val_loss: 3.9251 - val_acc: 0.1184\n",
      "Epoch 33/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1784 - acc: 0.0848 - val_loss: 3.9267 - val_acc: 0.1216\n",
      "Epoch 34/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.1466 - acc: 0.0839 - val_loss: 3.9076 - val_acc: 0.1193\n",
      "Epoch 35/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1469 - acc: 0.0832 - val_loss: 3.8961 - val_acc: 0.1255\n",
      "Epoch 36/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1623 - acc: 0.0818 - val_loss: 3.8960 - val_acc: 0.1275\n",
      "Epoch 37/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.1732 - acc: 0.0822 - val_loss: 3.8970 - val_acc: 0.1219\n",
      "Epoch 38/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1526 - acc: 0.0873 - val_loss: 3.8927 - val_acc: 0.1272\n",
      "Epoch 39/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1298 - acc: 0.0901 - val_loss: 3.8773 - val_acc: 0.1275\n",
      "Epoch 40/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.1425 - acc: 0.0876 - val_loss: 3.8686 - val_acc: 0.1294\n",
      "Epoch 41/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.1189 - acc: 0.0911 - val_loss: 3.8696 - val_acc: 0.1301\n",
      "Epoch 42/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 4.1212 - acc: 0.0922 - val_loss: 3.8760 - val_acc: 0.1265\n",
      "Epoch 43/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.1288 - acc: 0.0932 - val_loss: 3.8651 - val_acc: 0.1343\n",
      "Epoch 44/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.0790 - acc: 0.0966 - val_loss: 3.8526 - val_acc: 0.1324\n",
      "Epoch 45/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.0991 - acc: 0.0896 - val_loss: 3.8527 - val_acc: 0.1294\n",
      "Epoch 46/150\n",
      "112/112 [==============================] - 23s 204ms/step - loss: 4.0988 - acc: 0.1003 - val_loss: 3.8510 - val_acc: 0.1330\n",
      "Epoch 47/150\n",
      "112/112 [==============================] - 23s 205ms/step - loss: 4.0947 - acc: 0.0928 - val_loss: 3.8386 - val_acc: 0.1321\n",
      "Epoch 48/150\n",
      "112/112 [==============================] - 23s 207ms/step - loss: 4.0728 - acc: 0.0957 - val_loss: 3.8358 - val_acc: 0.1353\n",
      "Epoch 49/150\n",
      "112/112 [==============================] - 23s 207ms/step - loss: 4.0833 - acc: 0.1014 - val_loss: 3.8316 - val_acc: 0.1353\n",
      "Epoch 50/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.0720 - acc: 0.0940 - val_loss: 3.8382 - val_acc: 0.1373\n",
      "Epoch 51/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0865 - acc: 0.0954 - val_loss: 3.8269 - val_acc: 0.1396\n",
      "Epoch 52/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0436 - acc: 0.0991 - val_loss: 3.8181 - val_acc: 0.1402\n",
      "Epoch 53/150\n",
      "112/112 [==============================] - 23s 206ms/step - loss: 4.0649 - acc: 0.1049 - val_loss: 3.8206 - val_acc: 0.1386\n",
      "Epoch 54/150\n",
      "112/112 [==============================] - 23s 204ms/step - loss: 4.0740 - acc: 0.0993 - val_loss: 3.8133 - val_acc: 0.1425\n",
      "Epoch 55/150\n",
      "112/112 [==============================] - 23s 205ms/step - loss: 4.0476 - acc: 0.0990 - val_loss: 3.8090 - val_acc: 0.1376\n",
      "Epoch 56/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 4.0296 - acc: 0.1012 - val_loss: 3.8128 - val_acc: 0.1386\n",
      "Epoch 57/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.0332 - acc: 0.1056 - val_loss: 3.8072 - val_acc: 0.1415\n",
      "Epoch 58/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.0186 - acc: 0.1040 - val_loss: 3.8021 - val_acc: 0.1405\n",
      "Epoch 59/150\n",
      "112/112 [==============================] - 23s 204ms/step - loss: 4.0371 - acc: 0.1008 - val_loss: 3.8033 - val_acc: 0.1451\n",
      "Epoch 60/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0215 - acc: 0.1029 - val_loss: 3.7955 - val_acc: 0.1438\n",
      "Epoch 61/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0379 - acc: 0.1047 - val_loss: 3.7949 - val_acc: 0.1438\n",
      "Epoch 62/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0116 - acc: 0.1069 - val_loss: 3.7951 - val_acc: 0.1418\n",
      "Epoch 63/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 4.0087 - acc: 0.1059 - val_loss: 3.7945 - val_acc: 0.1490\n",
      "Epoch 64/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.0138 - acc: 0.1070 - val_loss: 3.7915 - val_acc: 0.1451\n",
      "Epoch 65/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 4.0178 - acc: 0.1058 - val_loss: 3.7871 - val_acc: 0.1454\n",
      "Epoch 66/150\n",
      "112/112 [==============================] - 23s 204ms/step - loss: 4.0028 - acc: 0.1044 - val_loss: 3.7792 - val_acc: 0.1490\n",
      "Epoch 67/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 4.0100 - acc: 0.1052 - val_loss: 3.7772 - val_acc: 0.1464\n",
      "Epoch 68/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9957 - acc: 0.1133 - val_loss: 3.7804 - val_acc: 0.1484\n",
      "Epoch 69/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 3.9954 - acc: 0.1007 - val_loss: 3.7731 - val_acc: 0.1503\n",
      "Epoch 70/150\n",
      "112/112 [==============================] - 23s 204ms/step - loss: 3.9933 - acc: 0.1050 - val_loss: 3.7741 - val_acc: 0.1490\n",
      "Epoch 71/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 3.9971 - acc: 0.1024 - val_loss: 3.7676 - val_acc: 0.1497\n",
      "Epoch 72/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9924 - acc: 0.1079 - val_loss: 3.7691 - val_acc: 0.1470\n",
      "Epoch 73/150\n",
      "112/112 [==============================] - 23s 205ms/step - loss: 3.9703 - acc: 0.1137 - val_loss: 3.7675 - val_acc: 0.1500\n",
      "Epoch 74/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.9790 - acc: 0.1097 - val_loss: 3.7724 - val_acc: 0.1506\n",
      "Epoch 75/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9912 - acc: 0.1093 - val_loss: 3.7674 - val_acc: 0.1467\n",
      "Epoch 76/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9645 - acc: 0.1120 - val_loss: 3.7660 - val_acc: 0.1477\n",
      "Epoch 77/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9700 - acc: 0.1153 - val_loss: 3.7600 - val_acc: 0.1451\n",
      "Epoch 78/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9695 - acc: 0.1148 - val_loss: 3.7621 - val_acc: 0.1493\n",
      "Epoch 79/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9591 - acc: 0.1113 - val_loss: 3.7570 - val_acc: 0.1519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9543 - acc: 0.1164 - val_loss: 3.7545 - val_acc: 0.1503\n",
      "Epoch 81/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 3.9514 - acc: 0.1189 - val_loss: 3.7558 - val_acc: 0.1457\n",
      "Epoch 82/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9579 - acc: 0.1109 - val_loss: 3.7519 - val_acc: 0.1493\n",
      "Epoch 83/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9753 - acc: 0.1107 - val_loss: 3.7542 - val_acc: 0.1503\n",
      "Epoch 84/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.9510 - acc: 0.1135 - val_loss: 3.7483 - val_acc: 0.1510\n",
      "Epoch 85/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9533 - acc: 0.1111 - val_loss: 3.7474 - val_acc: 0.1513\n",
      "Epoch 86/150\n",
      "112/112 [==============================] - 22s 201ms/step - loss: 3.9508 - acc: 0.1158 - val_loss: 3.7430 - val_acc: 0.1467\n",
      "Epoch 87/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9527 - acc: 0.1146 - val_loss: 3.7378 - val_acc: 0.1500\n",
      "Epoch 88/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9545 - acc: 0.1152 - val_loss: 3.7408 - val_acc: 0.1493\n",
      "Epoch 89/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.9261 - acc: 0.1195 - val_loss: 3.7416 - val_acc: 0.1529\n",
      "Epoch 90/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9566 - acc: 0.1153 - val_loss: 3.7374 - val_acc: 0.1513\n",
      "Epoch 91/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9479 - acc: 0.1114 - val_loss: 3.7386 - val_acc: 0.1513\n",
      "Epoch 92/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9168 - acc: 0.1175 - val_loss: 3.7401 - val_acc: 0.1467\n",
      "Epoch 93/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9532 - acc: 0.1178 - val_loss: 3.7400 - val_acc: 0.1470\n",
      "Epoch 94/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.9203 - acc: 0.1184 - val_loss: 3.7369 - val_acc: 0.1516\n",
      "Epoch 95/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.9322 - acc: 0.1177 - val_loss: 3.7369 - val_acc: 0.1532\n",
      "Epoch 96/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 3.9311 - acc: 0.1209 - val_loss: 3.7352 - val_acc: 0.1493\n",
      "Epoch 97/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9266 - acc: 0.1148 - val_loss: 3.7363 - val_acc: 0.1503\n",
      "Epoch 98/150\n",
      "112/112 [==============================] - 23s 203ms/step - loss: 3.9137 - acc: 0.1159 - val_loss: 3.7316 - val_acc: 0.1510\n",
      "Epoch 99/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9038 - acc: 0.1206 - val_loss: 3.7287 - val_acc: 0.1500\n",
      "Epoch 100/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9084 - acc: 0.1189 - val_loss: 3.7290 - val_acc: 0.1510\n",
      "Epoch 101/150\n",
      "112/112 [==============================] - 23s 202ms/step - loss: 3.9502 - acc: 0.1131 - val_loss: 3.7267 - val_acc: 0.1493\n",
      "Epoch 102/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9296 - acc: 0.1111 - val_loss: 3.7234 - val_acc: 0.1523\n",
      "Epoch 103/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9071 - acc: 0.1192 - val_loss: 3.7276 - val_acc: 0.1526\n",
      "Epoch 104/150\n",
      "112/112 [==============================] - 23s 201ms/step - loss: 3.9284 - acc: 0.1155 - val_loss: 3.7278 - val_acc: 0.1510\n",
      "Epoch 105/150\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 3.8626 - acc: 0.1205 - val_loss: 3.7255 - val_acc: 0.1510\n",
      "Epoch 00105: early stopping\n",
      "Test accuracy: 0.150961851977\n",
      "Creating model\n",
      "\n",
      "dropout= 0.742222761866006\n",
      "layers= 1\n",
      "dontFreeze= 1\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 22s 783ms/step - loss: 4.8900 - acc: 0.0091 - val_loss: 4.8048 - val_acc: 0.0176\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.7885 - acc: 0.0126 - val_loss: 4.6952 - val_acc: 0.0280\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.7563 - acc: 0.0150 - val_loss: 4.6595 - val_acc: 0.0385\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 19s 673ms/step - loss: 4.7367 - acc: 0.0219 - val_loss: 4.6086 - val_acc: 0.0489\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 19s 672ms/step - loss: 4.7065 - acc: 0.0261 - val_loss: 4.5442 - val_acc: 0.0496\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 19s 671ms/step - loss: 4.6835 - acc: 0.0310 - val_loss: 4.4830 - val_acc: 0.0587\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.6600 - acc: 0.0349 - val_loss: 4.4269 - val_acc: 0.0717\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.6187 - acc: 0.0410 - val_loss: 4.3585 - val_acc: 0.0776\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.5911 - acc: 0.0438 - val_loss: 4.3160 - val_acc: 0.0802\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.5564 - acc: 0.0506 - val_loss: 4.2884 - val_acc: 0.0802\n",
      "Epoch 11/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.5242 - acc: 0.0560 - val_loss: 4.2480 - val_acc: 0.0897\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.5155 - acc: 0.0527 - val_loss: 4.2202 - val_acc: 0.0910\n",
      "Epoch 13/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.4830 - acc: 0.0592 - val_loss: 4.1984 - val_acc: 0.0916\n",
      "Epoch 14/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.4605 - acc: 0.0650 - val_loss: 4.1477 - val_acc: 0.0985\n",
      "Epoch 15/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.4466 - acc: 0.0598 - val_loss: 4.1447 - val_acc: 0.1040\n",
      "Epoch 16/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.4144 - acc: 0.0683 - val_loss: 4.1311 - val_acc: 0.1056\n",
      "Epoch 17/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.4041 - acc: 0.0665 - val_loss: 4.1258 - val_acc: 0.1109\n",
      "Epoch 18/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.4079 - acc: 0.0700 - val_loss: 4.1146 - val_acc: 0.1082\n",
      "Epoch 19/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.3624 - acc: 0.0728 - val_loss: 4.0977 - val_acc: 0.1164\n",
      "Epoch 20/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.3757 - acc: 0.0741 - val_loss: 4.0869 - val_acc: 0.1122\n",
      "Epoch 21/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.3735 - acc: 0.0759 - val_loss: 4.0717 - val_acc: 0.1154\n",
      "Epoch 22/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.3352 - acc: 0.0772 - val_loss: 4.0685 - val_acc: 0.1118\n",
      "Epoch 23/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.3120 - acc: 0.0807 - val_loss: 4.0516 - val_acc: 0.1184\n",
      "Epoch 24/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.3368 - acc: 0.0792 - val_loss: 4.0482 - val_acc: 0.1157\n",
      "Epoch 25/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.3167 - acc: 0.0846 - val_loss: 4.0488 - val_acc: 0.1161\n",
      "Epoch 26/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.3097 - acc: 0.0828 - val_loss: 4.0360 - val_acc: 0.1187\n",
      "Epoch 27/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.2955 - acc: 0.0820 - val_loss: 4.0319 - val_acc: 0.1131\n",
      "Epoch 28/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.2908 - acc: 0.0885 - val_loss: 4.0351 - val_acc: 0.1161\n",
      "Epoch 29/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2582 - acc: 0.0890 - val_loss: 4.0303 - val_acc: 0.1118\n",
      "Epoch 30/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.2593 - acc: 0.0904 - val_loss: 4.0027 - val_acc: 0.1197\n",
      "Epoch 31/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.2704 - acc: 0.0906 - val_loss: 4.0107 - val_acc: 0.1206\n",
      "Epoch 32/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2710 - acc: 0.0885 - val_loss: 4.0081 - val_acc: 0.1171\n",
      "Epoch 33/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2729 - acc: 0.0894 - val_loss: 3.9835 - val_acc: 0.1272\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2445 - acc: 0.0907 - val_loss: 3.9825 - val_acc: 0.1246\n",
      "Epoch 35/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2249 - acc: 0.0926 - val_loss: 3.9781 - val_acc: 0.1285\n",
      "Epoch 36/150\n",
      "28/28 [==============================] - 19s 670ms/step - loss: 4.2090 - acc: 0.0975 - val_loss: 3.9743 - val_acc: 0.1278\n",
      "Epoch 37/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.2032 - acc: 0.0959 - val_loss: 3.9666 - val_acc: 0.1285\n",
      "Epoch 38/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.2068 - acc: 0.0952 - val_loss: 3.9741 - val_acc: 0.1210\n",
      "Epoch 39/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.2220 - acc: 0.0920 - val_loss: 3.9615 - val_acc: 0.1252\n",
      "Epoch 40/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.2171 - acc: 0.1006 - val_loss: 3.9535 - val_acc: 0.1246\n",
      "Epoch 41/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1941 - acc: 0.0961 - val_loss: 3.9568 - val_acc: 0.1324\n",
      "Epoch 42/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.1728 - acc: 0.0974 - val_loss: 3.9530 - val_acc: 0.1294\n",
      "Epoch 43/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1784 - acc: 0.0978 - val_loss: 3.9480 - val_acc: 0.1285\n",
      "Epoch 44/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.1987 - acc: 0.1018 - val_loss: 3.9491 - val_acc: 0.1255\n",
      "Epoch 45/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1811 - acc: 0.0984 - val_loss: 3.9369 - val_acc: 0.1281\n",
      "Epoch 46/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1571 - acc: 0.1038 - val_loss: 3.9454 - val_acc: 0.1288\n",
      "Epoch 47/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.1613 - acc: 0.0990 - val_loss: 3.9261 - val_acc: 0.1304\n",
      "Epoch 48/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1561 - acc: 0.1028 - val_loss: 3.9339 - val_acc: 0.1291\n",
      "Epoch 49/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1498 - acc: 0.1089 - val_loss: 3.9342 - val_acc: 0.1291\n",
      "Epoch 50/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1561 - acc: 0.1089 - val_loss: 3.9216 - val_acc: 0.1285\n",
      "Epoch 51/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1641 - acc: 0.1054 - val_loss: 3.9236 - val_acc: 0.1330\n",
      "Epoch 52/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1358 - acc: 0.1049 - val_loss: 3.9228 - val_acc: 0.1353\n",
      "Epoch 53/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1494 - acc: 0.1032 - val_loss: 3.9195 - val_acc: 0.1321\n",
      "Epoch 54/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1301 - acc: 0.1096 - val_loss: 3.9022 - val_acc: 0.1334\n",
      "Epoch 55/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1274 - acc: 0.1108 - val_loss: 3.9028 - val_acc: 0.1343\n",
      "Epoch 56/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.1488 - acc: 0.1050 - val_loss: 3.8984 - val_acc: 0.1373\n",
      "Epoch 57/150\n",
      "28/28 [==============================] - 19s 666ms/step - loss: 4.1207 - acc: 0.1037 - val_loss: 3.9074 - val_acc: 0.1356\n",
      "Epoch 58/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1280 - acc: 0.1039 - val_loss: 3.9074 - val_acc: 0.1360\n",
      "Epoch 59/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1112 - acc: 0.1132 - val_loss: 3.8911 - val_acc: 0.1366\n",
      "Epoch 60/150\n",
      "28/28 [==============================] - 19s 669ms/step - loss: 4.1000 - acc: 0.1131 - val_loss: 3.9013 - val_acc: 0.1343\n",
      "Epoch 61/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1221 - acc: 0.1114 - val_loss: 3.8885 - val_acc: 0.1360\n",
      "Epoch 62/150\n",
      "28/28 [==============================] - 19s 670ms/step - loss: 4.1051 - acc: 0.1156 - val_loss: 3.8882 - val_acc: 0.1321\n",
      "Epoch 63/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.1057 - acc: 0.1096 - val_loss: 3.8882 - val_acc: 0.1304\n",
      "Epoch 64/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.1061 - acc: 0.1087 - val_loss: 3.8804 - val_acc: 0.1340\n",
      "Epoch 65/150\n",
      "28/28 [==============================] - 19s 668ms/step - loss: 4.0921 - acc: 0.1122 - val_loss: 3.8820 - val_acc: 0.1366\n",
      "Epoch 66/150\n",
      "28/28 [==============================] - 19s 667ms/step - loss: 4.0920 - acc: 0.1115 - val_loss: 3.8912 - val_acc: 0.1321\n",
      "Epoch 00066: early stopping\n",
      "Test accuracy: 0.132050864041\n",
      "Creating model\n",
      "\n",
      "dropout= 0.7919157826643053\n",
      "layers= 0\n",
      "dontFreeze= 3\n",
      "batchSize= 256\n",
      "\n",
      "Epoch 1/150\n",
      "28/28 [==============================] - 22s 800ms/step - loss: 5.0622 - acc: 0.0092 - val_loss: 5.1047 - val_acc: 0.0231\n",
      "Epoch 2/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 4.9167 - acc: 0.0122 - val_loss: 4.5893 - val_acc: 0.0551\n",
      "Epoch 3/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.8112 - acc: 0.0203 - val_loss: 4.4612 - val_acc: 0.0730\n",
      "Epoch 4/150\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 4.7484 - acc: 0.0268 - val_loss: 4.3593 - val_acc: 0.0851\n",
      "Epoch 5/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.6879 - acc: 0.0379 - val_loss: 4.2724 - val_acc: 0.0955\n",
      "Epoch 6/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.6382 - acc: 0.0373 - val_loss: 4.2225 - val_acc: 0.1017\n",
      "Epoch 7/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.5949 - acc: 0.0394 - val_loss: 4.1736 - val_acc: 0.1053\n",
      "Epoch 8/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.5657 - acc: 0.0463 - val_loss: 4.1383 - val_acc: 0.1102\n",
      "Epoch 9/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.5140 - acc: 0.0524 - val_loss: 4.0833 - val_acc: 0.1122\n",
      "Epoch 10/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 4.4875 - acc: 0.0534 - val_loss: 4.0626 - val_acc: 0.1138\n",
      "Epoch 11/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.4547 - acc: 0.0640 - val_loss: 4.0449 - val_acc: 0.1157\n",
      "Epoch 12/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.4142 - acc: 0.0625 - val_loss: 4.0179 - val_acc: 0.1213\n",
      "Epoch 13/150\n",
      "28/28 [==============================] - 19s 684ms/step - loss: 4.3805 - acc: 0.0694 - val_loss: 3.9959 - val_acc: 0.1239\n",
      "Epoch 14/150\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 4.3442 - acc: 0.0776 - val_loss: 3.9745 - val_acc: 0.1311\n",
      "Epoch 15/150\n",
      "28/28 [==============================] - 19s 684ms/step - loss: 4.3387 - acc: 0.0719 - val_loss: 3.9632 - val_acc: 0.1275\n",
      "Epoch 16/150\n",
      "28/28 [==============================] - 19s 679ms/step - loss: 4.2898 - acc: 0.0782 - val_loss: 3.9471 - val_acc: 0.1239\n",
      "Epoch 17/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.2705 - acc: 0.0837 - val_loss: 3.9339 - val_acc: 0.1304\n",
      "Epoch 18/150\n",
      "28/28 [==============================] - 19s 680ms/step - loss: 4.2406 - acc: 0.0840 - val_loss: 3.9214 - val_acc: 0.1272\n",
      "Epoch 19/150\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 4.2339 - acc: 0.0861 - val_loss: 3.9086 - val_acc: 0.1311\n",
      "Epoch 20/150\n",
      "28/28 [==============================] - 19s 685ms/step - loss: 4.2050 - acc: 0.0914 - val_loss: 3.8945 - val_acc: 0.1288\n",
      "Epoch 21/150\n",
      "28/28 [==============================] - 19s 683ms/step - loss: 4.1880 - acc: 0.0881 - val_loss: 3.8815 - val_acc: 0.1337\n",
      "Epoch 22/150\n",
      "28/28 [==============================] - 19s 683ms/step - loss: 4.1715 - acc: 0.0921 - val_loss: 3.8725 - val_acc: 0.1347\n",
      "Epoch 23/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 4.1652 - acc: 0.0904 - val_loss: 3.8642 - val_acc: 0.1334\n",
      "Epoch 24/150\n",
      "28/28 [==============================] - 19s 681ms/step - loss: 4.1340 - acc: 0.0976 - val_loss: 3.8542 - val_acc: 0.1369\n",
      "Epoch 25/150\n",
      "28/28 [==============================] - 21s 733ms/step - loss: 4.1154 - acc: 0.0995 - val_loss: 3.8396 - val_acc: 0.1376\n",
      "Epoch 26/150\n",
      "28/28 [==============================] - 20s 732ms/step - loss: 4.0951 - acc: 0.1018 - val_loss: 3.8346 - val_acc: 0.1431\n",
      "Epoch 27/150\n",
      "28/28 [==============================] - 21s 751ms/step - loss: 4.0926 - acc: 0.1048 - val_loss: 3.8260 - val_acc: 0.1392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "28/28 [==============================] - 22s 795ms/step - loss: 4.0543 - acc: 0.1015 - val_loss: 3.8194 - val_acc: 0.1418\n",
      "Epoch 29/150\n",
      "28/28 [==============================] - 21s 738ms/step - loss: 4.0469 - acc: 0.1096 - val_loss: 3.8062 - val_acc: 0.1425\n",
      "Epoch 30/150\n",
      "28/28 [==============================] - 20s 724ms/step - loss: 4.0335 - acc: 0.1134 - val_loss: 3.8043 - val_acc: 0.1422\n",
      "Epoch 31/150\n",
      "28/28 [==============================] - 20s 717ms/step - loss: 4.0232 - acc: 0.1042 - val_loss: 3.7960 - val_acc: 0.1454\n",
      "Epoch 32/150\n",
      "28/28 [==============================] - 22s 800ms/step - loss: 4.0232 - acc: 0.1093 - val_loss: 3.7899 - val_acc: 0.1457\n",
      "Epoch 33/150\n",
      "28/28 [==============================] - 20s 729ms/step - loss: 3.9978 - acc: 0.1149 - val_loss: 3.7892 - val_acc: 0.1418\n",
      "Epoch 34/150\n",
      "28/28 [==============================] - 19s 685ms/step - loss: 3.9931 - acc: 0.1131 - val_loss: 3.7835 - val_acc: 0.1441\n",
      "Epoch 35/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 3.9823 - acc: 0.1117 - val_loss: 3.7761 - val_acc: 0.1457\n",
      "Epoch 36/150\n",
      "28/28 [==============================] - 22s 785ms/step - loss: 3.9964 - acc: 0.1128 - val_loss: 3.7723 - val_acc: 0.1474\n",
      "Epoch 37/150\n",
      "28/28 [==============================] - 20s 718ms/step - loss: 3.9612 - acc: 0.1233 - val_loss: 3.7659 - val_acc: 0.1487\n",
      "Epoch 38/150\n",
      "28/28 [==============================] - 19s 682ms/step - loss: 3.9288 - acc: 0.1232 - val_loss: 3.7683 - val_acc: 0.1461\n",
      "Epoch 39/150\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 3.9348 - acc: 0.1185 - val_loss: 3.7648 - val_acc: 0.1516\n",
      "Epoch 40/150\n",
      "28/28 [==============================] - 20s 706ms/step - loss: 3.9293 - acc: 0.1202 - val_loss: 3.7591 - val_acc: 0.1513\n",
      "Epoch 41/150\n",
      "28/28 [==============================] - 20s 713ms/step - loss: 3.9484 - acc: 0.1182 - val_loss: 3.7560 - val_acc: 0.1497\n",
      "Epoch 42/150\n",
      "28/28 [==============================] - 19s 688ms/step - loss: 3.9190 - acc: 0.1264 - val_loss: 3.7538 - val_acc: 0.1545\n",
      "Epoch 43/150\n",
      "28/28 [==============================] - 23s 813ms/step - loss: 3.8999 - acc: 0.1237 - val_loss: 3.7504 - val_acc: 0.1565\n",
      "Epoch 44/150\n",
      "28/28 [==============================] - 20s 722ms/step - loss: 3.8769 - acc: 0.1275 - val_loss: 3.7447 - val_acc: 0.1542\n",
      "Epoch 45/150\n",
      "28/28 [==============================] - 20s 723ms/step - loss: 3.8945 - acc: 0.1255 - val_loss: 3.7417 - val_acc: 0.1549\n",
      "Epoch 46/150\n",
      "28/28 [==============================] - 20s 717ms/step - loss: 3.8860 - acc: 0.1287 - val_loss: 3.7347 - val_acc: 0.1516\n",
      "Epoch 47/150\n",
      "28/28 [==============================] - 23s 827ms/step - loss: 3.8849 - acc: 0.1257 - val_loss: 3.7320 - val_acc: 0.1526\n",
      "Epoch 48/150\n",
      "28/28 [==============================] - 21s 739ms/step - loss: 3.8812 - acc: 0.1234 - val_loss: 3.7319 - val_acc: 0.1539\n",
      "Epoch 49/150\n",
      "28/28 [==============================] - 19s 688ms/step - loss: 3.8452 - acc: 0.1366 - val_loss: 3.7344 - val_acc: 0.1539\n",
      "Epoch 50/150\n",
      "28/28 [==============================] - 21s 736ms/step - loss: 3.8557 - acc: 0.1314 - val_loss: 3.7341 - val_acc: 0.1529\n",
      "Epoch 51/150\n",
      "28/28 [==============================] - 22s 793ms/step - loss: 3.8444 - acc: 0.1356 - val_loss: 3.7301 - val_acc: 0.1559\n",
      "Epoch 52/150\n",
      "28/28 [==============================] - 20s 729ms/step - loss: 3.8319 - acc: 0.1419 - val_loss: 3.7277 - val_acc: 0.1578\n",
      "Epoch 53/150\n",
      "28/28 [==============================] - 20s 721ms/step - loss: 3.8163 - acc: 0.1375 - val_loss: 3.7245 - val_acc: 0.1552\n",
      "Epoch 54/150\n",
      "28/28 [==============================] - 20s 726ms/step - loss: 3.8247 - acc: 0.1382 - val_loss: 3.7254 - val_acc: 0.1588\n",
      "Epoch 55/150\n",
      "28/28 [==============================] - 23s 809ms/step - loss: 3.8313 - acc: 0.1319 - val_loss: 3.7242 - val_acc: 0.1604\n",
      "Epoch 56/150\n",
      "28/28 [==============================] - 20s 723ms/step - loss: 3.8204 - acc: 0.1414 - val_loss: 3.7230 - val_acc: 0.1604\n",
      "Epoch 57/150\n",
      "28/28 [==============================] - 19s 689ms/step - loss: 3.8127 - acc: 0.1419 - val_loss: 3.7164 - val_acc: 0.1598\n",
      "Epoch 58/150\n",
      "28/28 [==============================] - 21s 749ms/step - loss: 3.8391 - acc: 0.1330 - val_loss: 3.7105 - val_acc: 0.1594\n",
      "Epoch 59/150\n",
      "28/28 [==============================] - 21s 768ms/step - loss: 3.8002 - acc: 0.1395 - val_loss: 3.7082 - val_acc: 0.1581\n",
      "Epoch 60/150\n",
      "28/28 [==============================] - 19s 694ms/step - loss: 3.7796 - acc: 0.1436 - val_loss: 3.7059 - val_acc: 0.1594\n",
      "Epoch 61/150\n",
      "28/28 [==============================] - 19s 692ms/step - loss: 3.7838 - acc: 0.1419 - val_loss: 3.7065 - val_acc: 0.1637\n",
      "Epoch 62/150\n",
      "28/28 [==============================] - 23s 829ms/step - loss: 3.7827 - acc: 0.1462 - val_loss: 3.7042 - val_acc: 0.1601\n",
      "Epoch 63/150\n",
      "28/28 [==============================] - 20s 708ms/step - loss: 3.8017 - acc: 0.1466 - val_loss: 3.7078 - val_acc: 0.1565\n",
      "Epoch 64/150\n",
      "28/28 [==============================] - 19s 685ms/step - loss: 3.7743 - acc: 0.1430 - val_loss: 3.7051 - val_acc: 0.1591\n",
      "Epoch 65/150\n",
      "28/28 [==============================] - 19s 689ms/step - loss: 3.7945 - acc: 0.1352 - val_loss: 3.6990 - val_acc: 0.1555\n",
      "Epoch 66/150\n",
      "28/28 [==============================] - 23s 809ms/step - loss: 3.7259 - acc: 0.1457 - val_loss: 3.6991 - val_acc: 0.1572\n",
      "Epoch 67/150\n",
      "28/28 [==============================] - 21s 737ms/step - loss: 3.7900 - acc: 0.1425 - val_loss: 3.7011 - val_acc: 0.1591\n",
      "Epoch 68/150\n",
      "28/28 [==============================] - 19s 693ms/step - loss: 3.7654 - acc: 0.1445 - val_loss: 3.6991 - val_acc: 0.1575\n",
      "Epoch 69/150\n",
      "28/28 [==============================] - 20s 715ms/step - loss: 3.7571 - acc: 0.1466 - val_loss: 3.6944 - val_acc: 0.1572\n",
      "Epoch 70/150\n",
      "28/28 [==============================] - 22s 792ms/step - loss: 3.7363 - acc: 0.1477 - val_loss: 3.6935 - val_acc: 0.1594\n",
      "Epoch 71/150\n",
      "28/28 [==============================] - 21s 737ms/step - loss: 3.7497 - acc: 0.1502 - val_loss: 3.6940 - val_acc: 0.1588\n",
      "Epoch 00071: early stopping\n",
      "Test accuracy: 0.158787088365\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='XceptionOptimization_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 6640/8000 [00:45<00:09, 146.71it/s]"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "val_loss, val_acc = best_model.evaluate(X_test, Y_test);\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(\"Validation loss: \", val_loss)\n",
    "print(\"Validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 90, 90, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 44, 44, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 44, 44, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 44, 44, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 42, 42, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 42, 42, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 42, 42, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 42, 42, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 42, 42, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 42, 42, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 42, 42, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 42, 42, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 21, 21, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 21, 21, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 21, 21, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 21, 21, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 21, 21, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 21, 21, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 21, 21, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 21, 21, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 21, 21, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 21, 21, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 11, 11, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 11, 11, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 11, 11, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 11, 11, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 11, 11, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 11, 11, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 11, 11, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 11, 11, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 11, 11, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 11, 11, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 6, 728)    186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 6, 6, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 6, 6, 728)    2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6, 6, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 6, 6, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 6, 6, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 6, 6, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 6, 6, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 6, 6, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 6, 6, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 6, 6, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 6, 6, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 6, 6, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 6, 6, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 6, 6, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 6, 6, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 6, 6, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 6, 6, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 6, 6, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 6, 6, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 6, 6, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 6, 6, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 6, 6, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 6, 6, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 6, 6, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 6, 6, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 6, 6, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 6, 6, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 6, 6, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 6, 6, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 6, 6, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 6, 6, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 6, 6, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 6, 6, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 6, 6, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 6, 6, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 6, 6, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 6, 6, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 6, 6, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 6, 6, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 6, 6, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 6, 6, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 6, 6, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 6, 6, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 6, 6, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 6, 6, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 6, 6, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 6, 6, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 6, 6, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 6, 6, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 6, 6, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 6, 6, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 6, 6, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 6, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 6, 6, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 6, 6, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 6, 6, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 6, 6, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 6, 6, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 6, 6, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 6, 6, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 6, 6, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 6, 6, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 6, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 6, 6, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 6, 6, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 6, 6, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 6, 6, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 6, 6, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 6, 6, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 6, 6, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 6, 6, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 6, 6, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 6, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 6, 6, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 6, 6, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 6, 6, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 6, 6, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 6, 6, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 6, 6, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 6, 6, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 6, 6, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 6, 6, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 6, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 6, 6, 728)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 6, 6, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 6, 6, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 6, 6, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 6, 6, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 6, 6, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 3, 3, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3, 3, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 3, 3, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 3, 3, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 3, 3, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 3, 3, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 3, 3, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 3, 3, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 3, 3, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18432)        0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18432)        0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         18875392    dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 120)          61560       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,323,232\n",
      "Trainable params: 22,625,400\n",
      "Non-trainable params: 17,697,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.save(modelPath+modelName);\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, accuracy is low here because we are not taking advantage of the pre-trained weights as they cannot be downloaded in the kernel. This means we are training the wights from scratch and I we have only run 1 epoch due to the hardware constraints in the kernel.\n",
    "\n",
    "Next we will make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.DataFrame(preds)\n",
    "# # Set column names to those generated by the one-hot encoding earlier\n",
    "# col_names = one_hot.columns.values\n",
    "# sub.columns = col_names\n",
    "# # Insert the column id from the sample_submission at the start of the data frame\n",
    "# sub.insert(0, 'id', df_test['id'])\n",
    "# sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
